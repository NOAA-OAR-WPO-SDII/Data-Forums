{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff320bf9",
   "metadata": {},
   "source": [
    "# Forum 2 - Hello Data, Meet Weather\n",
    "\n",
    "Author: Jonathon Mote, PhD - Weather Program Office\n",
    "September 2025\n",
    "\n",
    "This tutorial is designed for social scientists who want to explore how their own data might begin to interface with weather and hazard datasets.  The tutorial will provide a quick overview of Jupyter notebooks and tools, some geospatial tools, and API access for environmental and weather data.\n",
    "\n",
    "### Goals:\n",
    "1. Overview of API access\n",
    "2. Load and explore a sample of the Extreme Weather and Society Survey data\n",
    "3. Bring in a sample weather dataset (e.g., watches/warnings/advisories from NWS)\n",
    "4. Perform a simple merge\n",
    "5. Visualize a relationship between survey responses and weather\n",
    "\n",
    "Note: Ensure that the datasets are open access, or you have an API key for restricted access.  For this example, the datasets in this notebook do not require an API key.\n",
    "\n",
    "### Why do this?\n",
    "\n",
    "In general, the integration of weather and hazard data, and addressing both temporal and spatial resolutions, might allow for a deeper understanding of how people perceive, interpret, and respond to weather and climate risks in relation to the actual meteorological conditions they experience.  For this tutorial, a range of potential research questions can be posed:\n",
    "\n",
    "1. How accurately do people perceive the frequency or severity of extreme weather events in their area?\n",
    "2. Are there socioeconomic or demographic predictors of inaccurate weather risk perception?\n",
    "3. Do people who recall receiving more weather warnings perceive greater weather risk?\n",
    "4. Is there a mismatch between how meteorologists frame risk (as understood by watches and warnings) and how the public interprets it?\n",
    "5. Are individuals who have experienced extreme weather more likely to change their behavior or risk perception?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867feb-2ffc-4d5e-9e14-3d4f277525e9",
   "metadata": {},
   "source": [
    "### Introduction and Setup (Imports)\n",
    "\n",
    "In Jupyter, there are a large number of python-based \"libraries\" that help with data loading, transformation, and analysis.  These libraries provide tools that help us do things like make graphs, work with data, or do more complex calculations, like regression.  It's good practice to have all libraries imported at the beginning.  You can always add (even install) libraries as you go along, you just have to rerun the cells (or restart the kernel if a new install).\n",
    "\n",
    "In this tutorial, the primary libraries we will use are:\n",
    "\n",
    "- **pandas**: For working with tabular data in DataFrames.  It is commonly imported with an alias (pd), so we don't constantly have to type out pandas.\n",
    "- **requests**: For easily fetching data from web APIs and URLs (using GET, POST, etc).\n",
    "- **timedelta**: Imported from the datetime library, for representing time intervals.\n",
    "- **BytesIO**: Imported from the IO library, for treating in-memory bytes like a file for reading.\n",
    "- **geopandas**: To work with geospatial data, allowing us to perform spatial operations and handle geometries such as points, polygons, and lines.\n",
    "- **Point**: Imported from Shapely, for creating geometric points for mapping.\n",
    "- **Pyplot**: Imported from MatPlotLib using the alias \"plt\", for making simple customizable  plots and charts.\n",
    "- **Seaborn**: Imported using the alias \"sns\", for creating statistical and more visually appealing plots. \n",
    "- **time**: Provides functions for working with time, such as measuring durations, pausing executions, and accessing system time.\n",
    "- **tqdm**: Adds progress bars to loops for tracking execution.\n",
    "- **scipy.stats.chi2_contingency**: Imported from Scipy, to run a chi-square test of independence to check if two categorical variables are related.\n",
    "- **statsmodels.api as sm**: Imported with the alias \"sm\", it provides tools for statistical models, including logistic regression.\n",
    "- **OrderedModel**: Imported from statsmodels, used for ordinal logistic regression models when outcomes are ordered categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691dcae-b33c-4ab8-917d-da4b7f2de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "\n",
    "# geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#utilities\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#statistical modeling/inference\n",
    "from scipy.stats import chi2_contingency        \n",
    "import statsmodels.api as sm                    \n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdde11",
   "metadata": {},
   "source": [
    "## Step 1: Search for Datasets\n",
    "\n",
    "In this step, we will explore API access to a data repository, the Harvard Dataverse.  An API (Application Programming Interface) is just a set of rules and tools that allows different software to communication and interact with each other.  In this case, we want our Jupyter notebook to interact with the server for information on datasets.  We use the python library \"Requests\" to simplify and automate our requests, and Dataverse returns what we requested (hopefully), typically in a format called JSON.  We then use Pandas to transform the JSON in a dataframe, making the results easier to read and manipulate.  \n",
    "\n",
    "- **Note**: Not all APIs are created equally and there might be differences across repositories.  Check each API's documentation for how to get started, authentication, search and data access, and more.  For Harvard's Dataverse, the [Dataverse API Guide](https://guides.dataverse.org/en/latest/api/index.html) is a comprehensive, up-to-date documentation for all operations in Harvard’s Dataverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b78722-de05-4332-bd49-265d741a5cc0",
   "metadata": {},
   "source": [
    "##### Simple search for 10 results\n",
    "\n",
    "By default, the Harvard Dataverse only returns 10 results per search request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037550f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires requests and pandas\n",
    "\n",
    "# Define search query\n",
    "query = \"ripberger\"\n",
    "search_url = f\"https://dataverse.harvard.edu/api/search?q={query}&type=dataset\"\n",
    "\n",
    "# Perform search\n",
    "response = requests.get(search_url)\n",
    "results = response.json()\n",
    "\n",
    "# Convert items to DataFrame\n",
    "items = results['data']['items']\n",
    "df_results = pd.DataFrame(items)\n",
    "\n",
    "# Show key columns\n",
    "#df_results[['name', 'global_id', 'published_at', 'citation']]\n",
    "df_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cf11b-d272-4823-bb22-001368af08b2",
   "metadata": {},
   "source": [
    "##### Search with more than 10 results\n",
    "\n",
    "We can make an API call that goes beyond the 10 result limit by creating a loop.  The \"while True\" statement will continue running (10 results at a time) until there are no results remaining.  We collect all of the results in one list using the \"extend\" command.  Finally, we can limit the dataframe to only view a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba6ce1-293f-4dd2-90a5-57cf01e33690",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ripberger\"\n",
    "start = 0\n",
    "per_page = 20  # Max per page is 100\n",
    "all_items = []\n",
    "\n",
    "while True:\n",
    "    search_url = (\n",
    "        f\"https://dataverse.harvard.edu/api/search?\"\n",
    "        f\"q={query}&type=dataset&start={start}&per_page={per_page}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    items = data['data']['items']\n",
    "    all_items.extend(items)\n",
    "    \n",
    "    # Break if fewer than per_page results are returned (i.e., last page)\n",
    "    if len(items) < per_page:\n",
    "        break\n",
    "    start += per_page\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(all_items)\n",
    "df_results[['name', 'global_id', 'published_at', 'citation']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cf708-914f-48b5-b581-a65ad628c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many datasets?  Each row (first number) represents a dataset.\n",
    "df_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3ced3-5580-48d3-bfbd-10617e4532db",
   "metadata": {},
   "source": [
    "### Subsetting Our Results\n",
    "\n",
    "Let's say we don't want all of these, but only a subset of related surveys.  For this step, and the remainder of notebook, we will focus on the yearly waves of the Extreme Weather and Society Survey (WXYY). So let's subset them the dataframe from the earlier API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0f9d5-63bc-4892-8a4f-88bf2f8550bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset names you want to filter on\n",
    "target_names = [\"WX17\", \"WX18\", \"WX19\", \"WX20\", \"WX21\", \"WX22\", \"WX23\", \"WX24\"]\n",
    "\n",
    "# Subset the dataframe\n",
    "df_subset = df_results[df_results['name'].isin(target_names)]\n",
    "\n",
    "# Display the result\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb964d7",
   "metadata": {},
   "source": [
    "## Step 2: Get Dataset Metadata and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584267c4-9f99-4f97-8881-1c095a6b8f5c",
   "metadata": {},
   "source": [
    "### Getting metadata for a single dataset\n",
    "\n",
    "Let's examine the metadata for one of the datasets in the subset, WX18.  The Dataverse uses it's own API file metadata.  First, we will look at file-level metadata.  Next, we will pull the full dataset metadata.  Since the results will be in JSON, we will convert that to a flat file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85380a77-04f2-469a-b550-e34c16b25259",
   "metadata": {},
   "source": [
    "### File level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row [0] by position\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get dataset metadata\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Display list of files\n",
    "files = metadata_response['data']['latestVersion']['files']\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d8ce9-6a2d-4cc8-a053-5e10f8a04a0e",
   "metadata": {},
   "source": [
    "### Full dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f4f8b-9f6a-4485-955c-f81dde9bc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get full dataset metadata (latest version)\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/versions/:latest?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Display the JSON\n",
    "metadata_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7019893-9ffe-47f5-97e9-09bca1ceb354",
   "metadata": {},
   "source": [
    "### Transform the results from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dd163-71e0-4c87-a24b-5bf42d3609d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten JSON into a single row using json_normalize\n",
    "df_meta = pd.json_normalize(metadata_response)\n",
    "\n",
    "# Transpose so keys become a column and values another\n",
    "df_meta_t = df_meta.T.reset_index()\n",
    "df_meta_t.columns = [\"field\", \"value\"]\n",
    "\n",
    "df_meta_t.head(20)  # show first 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f684723-3e20-4f78-9766-c90173a6bafc",
   "metadata": {},
   "source": [
    "### Getting metadata for multiple datasets\n",
    "\n",
    "Let's say we want the file-level metadata for all waves.  Against, we set up a loop call, to loop through all of the files using the persistent ID (DOI) and pick up the metadata information we want.  This time, we will limit the items we want to see (as seen in the \"for f in files\" loop below) and have the results formatted into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db396a02-8713-4d18-ba36-9383808e669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold all file metadata\n",
    "all_files = []\n",
    "\n",
    "# Loop through persistent IDs\n",
    "for pid in df_subset['global_id']:\n",
    "    metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={pid}\"\n",
    "    response = requests.get(metadata_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        metadata = response.json()\n",
    "        files = metadata['data']['latestVersion']['files']\n",
    "        \n",
    "        for f in files:\n",
    "            file_info = {\n",
    "                'dataset_title': metadata['data']['latestVersion']['metadataBlocks']['citation']['fields'][0]['value'],\n",
    "                'file_id': f['dataFile']['id'],\n",
    "                'file_label': f['label'],\n",
    "                'file_size': f['dataFile'].get('filesize', None),\n",
    "                'file_description': f.get('description', ''),\n",
    "                'persistent_id': pid\n",
    "            }\n",
    "            all_files.append(file_info)\n",
    "    \n",
    "    # Be respectful of API limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_files = pd.DataFrame(all_files)\n",
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc278427",
   "metadata": {},
   "source": [
    "## Step 3: Download a File\n",
    "\n",
    "Above, we see that each dataset file (.tab) is accompanied by PDFs of the instrument and a reference report.\n",
    "\n",
    "Let's download the dataset file (.tab) for first year of the survey, WX18, which has a file_id of \"3657710\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a111b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File ID for WxEM_Wave1.tab\n",
    "file_id = 3657710\n",
    "\n",
    "# Download directly to memory\n",
    "file_url = f\"https://dataverse.harvard.edu/api/access/datafile/{file_id}?format=original\"\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# Load into pandas directly from memory, assuming comma-delimited content\n",
    "df_18 = pd.read_csv(BytesIO(response.content), sep=',', encoding='ISO-8859-1', engine='python', on_bad_lines='skip')\n",
    "df_18.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d029e2f-c5d5-4a96-b137-8a8d77ba7078",
   "metadata": {},
   "source": [
    "### Examine the dataset\n",
    "\n",
    "Pandas has a number of attributes that can be used to examine characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b165-638f-4f8f-86c7-5baaf39002bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See basic shape of the data (rows, columns).  Each row represents a respondent.\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada7a0f-3378-4f42-a588-383ac6530e3b",
   "metadata": {},
   "source": [
    "#### Variable names\n",
    "\n",
    "Listing the columns shows all the variable names contained in the data.  To integrate with weather data, we are most interested in locating possible ways to join the data.  Typically, geographic variables are a good start.  In this survey, some good possible variables are state, zip, lat/lon.  These are pretty straightforward, but what about \"nws_region\"?  Does it contain WFOs?  Let's examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0012e2-7334-492d-8dd6-9026a265fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all column names\n",
    "df_18.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9502-1d22-4520-a342-1035a7c5b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is nws_region usefl at all?\n",
    "df_18['nws_region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79825058-2652-4f7e-a37e-d2f3e71494a7",
   "metadata": {},
   "source": [
    "##### Unfortunately, \"nws_region\" only has four regions.  Nonetheless, it might be useful at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d675f60",
   "metadata": {},
   "source": [
    "## Step 4: Merge with Weather Data\n",
    "\n",
    "In this example, we will extract warnings/watches/advisories from the Iowa Mesonet for each respondent three days prior to the start of the survey.  I know, that's pretty arbitrary but perhaps we're interested in the impact of the weather on responses to a weather survey.\n",
    "\n",
    "For each respondent, we will use their:\n",
    "\n",
    "1. lat, lon, and begin_date\n",
    "2. Query the IEM for any warnings/watches/advisories issued within 3 days prior to begin_date\n",
    "3. Store or summarize those results (e.g., number of WWAs, types, text, etc.)\n",
    "\n",
    "Please note the API Endpoint (Rest-like) and the structure of the request for those variables:\n",
    "\n",
    "https://mesonet.agron.iastate.edu/vtec/json.php?lon={lon}&lat={lat}&sdate={start}&edate={end}\n",
    "\n",
    "Remember, you can always check the API documentation for guidance: the [Iowa Mesonet API Guide](https://mesonet.agron.iastate.edu/api/).\n",
    "\n",
    "We will use this API call to loop through all 3,000 respondents and gather their information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982baaf3-b0e5-4923-9af9-dc5cdf42695b",
   "metadata": {},
   "source": [
    "### Iowa Mesonet \n",
    "\n",
    "So I'm just going to use lat/lon and the date to directly get watches/warnings/advisories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03230106-daee-42e8-99d1-8e635f846478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure begin_date is in datetime format\n",
    "df_18['begin_date'] = pd.to_datetime(df_18['begin_date'])\n",
    "\n",
    "# New column to store list of WWA names\n",
    "df_18['wwa_names'] = None\n",
    "\n",
    "# Loop through each respondent\n",
    "for idx, row in tqdm(df_18.iterrows(), total=len(df_18)):\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    end_date = row['begin_date']\n",
    "    start_date = end_date - timedelta(days=3)\n",
    "\n",
    "    # Build API URL\n",
    "    url = (\n",
    "        f\"https://mesonet.agron.iastate.edu/json/vtec_events_bypoint.py\"\n",
    "        f\"?lat={lat}&lon={lon}&sdate={start_date.strftime('%Y-%m-%d')}&edate={end_date.strftime('%Y-%m-%d')}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            names = [event['name'] for event in data.get('events', [])]\n",
    "            df_18.at[idx, 'wwa_names'] = names\n",
    "        else:\n",
    "            df_18.at[idx, 'wwa_names'] = []\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for idx={idx}, lat={lat}, lon={lon}: {e}\")\n",
    "        df_18.at[idx, 'wwa_names'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba1ca0-c11e-40cb-8fe0-939eea5e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's examine what we just collected.  We use the attribute \"dropna\" to make sure that there are no rows that empty (i.e., we didn't screw something up).\n",
    "df_18[['lat', 'lon', 'begin_date', 'wwa_names']].dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfd706-64e8-4b5d-8fe8-c1f7c136723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make sure we have the same number of row that we started with\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479299b-22fb-4e6b-9968-58ce1f5ac62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many respondents experienced watches/warnings/advisories?\n",
    "df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb31113-610d-4890-a714-6cd4fee795e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#who received a tornado warning?\n",
    "df_18[df_18['wwa_names'].apply(lambda x: 'Tornado Warning' in x if isinstance(x, list) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fecd87-078a-43f9-88ff-54b947ac89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#who received more than two?\n",
    "df_18[\n",
    "    df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 2)\n",
    "][['lat', 'lon', 'begin_date', 'wwa_names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead4b5-ceb4-4e74-8ab1-bfa05ecd9034",
   "metadata": {},
   "source": [
    "### Quick Visualizations of the Watches and Warnings\n",
    "\n",
    "Here are some quick descriptive visualizations to view the data.\n",
    "1. A geomap of survey respondents by number of watches and warnings 3 days prior to the survey.\n",
    "2. A bar chart showing frequency of types of watches and warnings.\n",
    "3. A line graph showing frequency of types of watches and warning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3200f-03e3-4ccd-ba8d-1ee1f1c5e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure lat/lon are float (just in case)\n",
    "df_18['lat'] = pd.to_numeric(df_18['lat'], errors='coerce')\n",
    "df_18['lon'] = pd.to_numeric(df_18['lon'], errors='coerce')\n",
    "\n",
    "# Step 2: Create geometry from lat/lon\n",
    "geometry = [Point(xy) for xy in zip(df_18['lon'], df_18['lat'])]\n",
    "gdf_18 = gpd.GeoDataFrame(df_18, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Step 3: Count WWAs\n",
    "gdf_18['wwa_count'] = gdf_18['wwa_names'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Step 4: Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "gdf_18.plot(column='wwa_count', cmap='OrRd', legend=True, ax=ax, markersize=10)\n",
    "ax.set_title(\"Survey Respondents by Number of WWAs (3 Days Prior)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb67ea-8ce5-4ce5-911f-2fa3f0dee664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and count\n",
    "all_names = df_18['wwa_names'].dropna().explode()\n",
    "top_names = Counter(all_names).most_common(10)\n",
    "\n",
    "# Plot\n",
    "names, counts = zip(*top_names)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(names[::-1], counts[::-1])\n",
    "plt.title(\"Top 10 Most Frequent WWAs\")\n",
    "plt.xlabel(\"Number of Respondents Exposed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b5893-7286-410b-a6c3-594d62c40043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count WWAs per date\n",
    "timeline = (\n",
    "    df_18[['begin_date', 'wwa_names']]\n",
    "    .dropna()\n",
    "    .assign(wwa_count=lambda df: df['wwa_names'].apply(len))\n",
    "    .groupby('begin_date')['wwa_count']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Plot\n",
    "timeline.plot(marker='o', figsize=(10, 4), title='Total WWAs by Survey Date')\n",
    "plt.ylabel('Total Warnings/Watch Events')\n",
    "plt.xlabel('Survey Date')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c6a06-2fd5-48eb-bb08-46818c8aaa24",
   "metadata": {},
   "source": [
    "### Visualizing WWWAs by Survey Reponses\n",
    "\n",
    "For a next step, we might do some quick analyses/visualizations of WWAs by survey response.  Perhaps even make an interactive version with a dropdown of survey responses that a user can play with?\n",
    "\n",
    "Let's say we want to examine how the presence of recent NWS advisories (3 days prior to taking the survey) correlates with survey respondents' perceived risk of that hazard.  How can we do that?\n",
    "\n",
    "Step 1.  Create two groups of respondents, those experienced a WWA prior to the survey and those who did not.\n",
    "Step 2.  Extract risk perception scores for a hazard (For risk_flood: 1-No risk, 2-Low Risk.....5-Extreme risk)\n",
    "Step 3.  Create comparative visualizations\n",
    "\n",
    "For these visualizations, we will focus on exposure to flood watches and warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211e79f-df3e-4b56-9d2d-83c13af46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prep: robust WWA exposure + ordered risk labels ---\n",
    "\n",
    "# 1) robust flood_wwa_exposure (handles NaN/non-lists)\n",
    "flood_wwa_keywords = ['Flood Advisory', 'Flood Warning', 'Flash Flood Warning', 'Flash Flood Watch', 'Flood Watch']\n",
    "\n",
    "def has_flood_wwa(wwas):\n",
    "    if isinstance(wwas, (list, tuple, set)):\n",
    "        return any(k in wwas for k in flood_wwa_keywords)\n",
    "    return False\n",
    "\n",
    "df_18['flood_wwa_exposure'] = df_18['wwa_names'].apply(has_flood_wwa)\n",
    "\n",
    "# 2) numeric -> labeled flood risk (ordered categorical)\n",
    "label_map = {1: 'No risk', 2: 'Low risk', 3: 'Moderate risk', 4: 'High risk', 5: 'Extreme risk'}\n",
    "ordered_risk_labels = list(label_map.values())\n",
    "\n",
    "# Coerce to numeric, map to labels, and set categorical order\n",
    "df_18['risk_flood_num'] = pd.to_numeric(df_18.get('risk_flood'), errors='coerce')\n",
    "df_18['risk_flood_label'] = pd.Categorical(\n",
    "    df_18['risk_flood_num'].map(label_map),\n",
    "    categories=ordered_risk_labels,\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084549b3-555b-44c8-86af-c9c840ff4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(\n",
    "    data=df_18,\n",
    "    x='risk_flood_label',\n",
    "    hue='flood_wwa_exposure',\n",
    "    order=ordered_risk_labels,\n",
    "    hue_order=[False, True]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Perceived Flood Risk\")\n",
    "plt.ylabel(\"Number of Respondents\")\n",
    "plt.title(\"Perceived Flood Risk by Exposure to Flood-Related WWAs\")\n",
    "plt.legend(title=\"WWA Exposure\", labels=[\"No\", \"Yes\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e72242-80ba-4658-9c5c-b89c04c71d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab -> proportions by exposure\n",
    "crosstab = pd.crosstab(\n",
    "    df_18['flood_wwa_exposure'],\n",
    "    df_18['risk_flood_label'],\n",
    "    normalize='index'\n",
    ")\n",
    "\n",
    "# ensure consistent order of rows/columns even if some levels are missing\n",
    "crosstab = crosstab.reindex(index=[False, True], columns=ordered_risk_labels)\n",
    "\n",
    "ax = crosstab.T.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "\n",
    "plt.title('Flood Risk Perception by WWA Exposure (Proportions)')\n",
    "plt.xlabel('Perceived Flood Risk')\n",
    "plt.ylabel('Proportion of Respondents')\n",
    "plt.legend(title='Exposed to Flood WWA', labels=['No', 'Yes'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257784d-91b7-495d-b2a7-87e6b8a29518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't personally like violin plots, but let's take a look\n",
    "sns.violinplot(data=df_18, x='flood_wwa_exposure', y='risk_flood', inner='quartile')\n",
    "plt.xticks([0, 1], ['No WWA Exposure', 'WWA Exposure'])\n",
    "plt.xlabel(\"Exposure to Flood WWA\")\n",
    "plt.ylabel(\"Perceived Flood Risk (1-5)\")\n",
    "plt.title(\"Distribution of Flood Risk Perception by WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b31535-6c89-4c43-935b-f1d5b7934c54",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "We're not limited to visualizations, we can explore a number of statistical procedures to really test our question of whether exposure to warnings and watches has an impact on survey responses.  For this tutorial, we will quickly conduct the following:\n",
    "\n",
    "1. Chi-Square: To evaluate whether exposure to flood-related warnings and watches and perceived flood risk categories are independent, with the p-value indicating if the observed relationship is statistically significant.\n",
    "2. Logistic Regression: To evaluate whether exposure to flood-related warnings and watches increases the odds of respondents reporting high or extreme flood risk compared to lower risk levels.\n",
    "3. Ordinal Logistic Regression: To evaluate whether exposure to flood-related warnings and watches shifts respondents toward higher categories of perceived flood risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48102616-f3ad-47b9-a863-48aff2a5efd3",
   "metadata": {},
   "source": [
    "### Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9805ec1-577e-4e29-bf8b-c58571dd9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table: WWA exposure vs. risk perception\n",
    "contingency = pd.crosstab(df_18['flood_wwa_exposure'], df_18['risk_flood_label'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\"Chi-square test\")\n",
    "print(f\"Chi2 = {chi2:.2f}, df = {dof}, p = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d836569-37c4-497e-9939-431d6ac8fdc0",
   "metadata": {},
   "source": [
    "This suggests that the distribution of flood risk perceptions is not independent of WWA exposure — in other words, people who were exposed to flood-related WWAs responded differently (in terms of risk levels) than those who were not exposed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3943fc-95ab-4b32-ace2-8a0f376bf859",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60e3b-5a33-4a9f-9d0f-d82366e42625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary outcome: high risk (≥4) vs. lower\n",
    "df_18['high_risk'] = df_18['risk_flood_num'] >= 4\n",
    "\n",
    "# Predictor: WWA exposure (cast to int)\n",
    "X = sm.add_constant(df_18['flood_wwa_exposure'].astype(int))\n",
    "y = df_18['high_risk'].astype(int)\n",
    "\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2c04a-7e01-4624-aea1-e55ed519613c",
   "metadata": {},
   "source": [
    "This suggests that respondents exposed to a flood WWA had significantly higher odds (about 39% greater, exp(0.3326) ≈ 1.39) of reporting high or extreme flood risk compared to those not exposed, though the overall model explains only a small share of variation in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c6f48-4a63-4943-8ee3-71812abf60ba",
   "metadata": {},
   "source": [
    "### Ordinal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4dc504-1897-44b6-bf0b-5222f52dae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs to avoid issues\n",
    "df_ord = df_18.dropna(subset=['risk_flood_num', 'flood_wwa_exposure'])\n",
    "\n",
    "# Predictor must be numeric (int)\n",
    "X = df_ord[['flood_wwa_exposure']].astype(int)\n",
    "\n",
    "# Outcome is ordered categories (numeric risk levels already ordered 1–5)\n",
    "y = df_ord['risk_flood_num']\n",
    "\n",
    "# Fit ordinal logistic regression\n",
    "mod = OrderedModel(y, X, distr='logit')\n",
    "res = mod.fit(method='bfgs')\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262f9dd-e592-410f-9d1b-9b053e8c4004",
   "metadata": {},
   "source": [
    "This suggests that respondents exposed to flood-related warnings and watches had a significantly higher likelihood of placing themselves in higher flood risk perception categories (coef = 0.296, p < 0.001), indicating a consistent upward shift in perceived risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6a768-069c-4eda-8c8f-0bcdee2245de",
   "metadata": {},
   "source": [
    "## But Did WWAs *Really* Affect Risk Perception?\n",
    "\n",
    "So it appears that exposure to watches and warnings has a statistically significant impact on survey responses.  But by how much?  For this, let's take a look at the predicted probabilities of survey responses.  This way, we can have an answer for “How much does exposure to a WWA change the probability of a respondent reporting ‘High’ or ‘Extreme’ flood risk?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3bdae-4502-4d07-8e19-5389cbb76050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two scenarios: no exposure (0) and exposure (1)\n",
    "scenarios = pd.DataFrame({\n",
    "    'flood_wwa_exposure': [0, 1]\n",
    "})\n",
    "\n",
    "# Predict probabilities for each risk category\n",
    "pred_probs = res.predict(scenarios)\n",
    "\n",
    "# Attach labels\n",
    "pred_probs.index = ['No Exposure', 'Exposure']\n",
    "pred_probs.columns = [f\"Risk {c}\" for c in pred_probs.columns]\n",
    "\n",
    "pred_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a547651-6f6e-42a7-9d62-5716cc99e669",
   "metadata": {},
   "source": [
    "## A possible interpretation\n",
    "\n",
    "Respondents who were exposed to a flood WWA were less likely to report no risk (10% → 8%) or low risk (32% → 28%), and more likely to report higher levels of risk perception, particularly at the “High” (15% → 18%) and “Extreme” (9% → 11%) categories. While the percentage point changes may look modest, they indicate a clear upward shift in perceived flood risk among those who received WWAs.  In other words, I think it is possible to say that exposure to a flood warning nudged people away from saying ‘no risk’ and toward saying ‘high or extreme risk.  However, with only 3,000 responses, I probably wouldn't.  But, as the academics say, this requires further study. 😉\n",
    "\n",
    "Now let's take a look at a visualization of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d88ad-5778-4c2a-8c1f-88a3a3da1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see HOW the probability distribution shift\n",
    "ax = pred_probs.T.plot(kind='bar', figsize=(10,6))\n",
    "plt.title(\"Predicted Flood Risk Perception by WWA Exposure\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Perceived Flood Risk Level\")\n",
    "plt.legend(title=\"WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e86aa-c61b-4c40-ad08-13ec49961f36",
   "metadata": {},
   "source": [
    "## Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5570c5-de5c-4d01-8741-1fb8b8f7e3ba",
   "metadata": {},
   "source": [
    "In this tutorial, we explored how to bring together **social survey data** and **weather warning data** to better understand how hazard information influences perceptions of risk. Using the Jupyter Notebook environment, you learned how to:  \n",
    "\n",
    "- **Work with APIs** to search for, access, and download data programmatically  \n",
    "- Organize and explore datasets interactively using **pandas**  \n",
    "- Merge survey responses with external data from the **Iowa Environmental Mesonet**  \n",
    "- Apply **geospatial tools** to handle location-based data  \n",
    "- Create clear, reproducible **visualizations** directly alongside your analysis  \n",
    "- Document your process in a way that combines code, results, and explanation all in one place  \n",
    "\n",
    "By the end of this notebook, you’ve seen how Jupyter can serve as both a **research lab and a communication tool**—a space where you can experiment with data, visualize results, and explain what you find.  \n",
    "\n",
    "### Moving Forward  \n",
    "- Try adapting this workflow to other survey topics (e.g., heat, drought, tornado risk)  \n",
    "- Explore additional Mesonet or NOAA APIs to enrich your analysis with different kinds of data  \n",
    "- Use Jupyter notebooks to build **reproducible reports**, where readers can see not just your conclusions but also the steps you took to get there  \n",
    "- Share your notebooks with collaborators as a way to make your analysis **transparent and interactive**  \n",
    "\n",
    "Ultimately, the key takeaway is that with just a few tools—**APIs, pandas, geospatial libraries, and Jupyter notebooks**—you can connect diverse datasets, analyze them in context, and tell meaningful data stories about risk and society.  \n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for following along!**  \n",
    "We encourage you to take this workflow and apply it to your own research questions about weather, risk, and society—the more you explore, the more insights you’ll uncover.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672add7a-975a-4423-8293-75be83a9ea90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
