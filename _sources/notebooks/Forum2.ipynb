{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff320bf9",
   "metadata": {},
   "source": [
    "# Forum 2 - From Metadata to Insights\n",
    "\n",
    "Author: Jonathon Mote, PhD - Weather Program Office\n",
    "September 2025\n",
    "\n",
    "This tutorial is designed for social scientists who want to explore how their own data might begin to interface with weather and hazard datasets.  The tutorial will provide a quick overview of Jupyter notebooks and tools, some geospatial tools, and the use of APIs to access data.\n",
    "\n",
    "### What we'll do in the notebook:\n",
    "1.  **Work with APIs** to search for, access, and download data programmatically\n",
    "2.  Organize, explore, and download datasets interactively using python libraries like **pandas** and **requests**.\n",
    "3.  Merge survey data with external data from the **Iowa Environmental Mesonet**\n",
    "4.  Apply **geospatial tools** to handle location-based data\n",
    "5.  Create clear, reproducible **visualizations** and **statistical analyses** directly alongside your analysis\n",
    "6.  Document our process in a way that combines code, results, and explanation all in one place  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f3885-3b64-4fd2-a631-338141a481e5",
   "metadata": {},
   "source": [
    "### The Steps We Will Follow\n",
    "\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; gap:14px; margin:14px 0 6px 0;\">\n",
    "\n",
    "  <!-- Step 1 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#d0eff8; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 1: Use an API to explore a repository, view metadata, and \"pull\" social data\n",
    "  </div>\n",
    "  <div style=\"width:2px; height:28px; background:#bfbfbf; border-radius:1px;\"></div>\n",
    "\n",
    "  <!-- Step 2 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#0069af; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 2: Use an API to \"pull\" weather data\n",
    "  </div>\n",
    "  <div style=\"width:2px; height:28px; background:#bfbfbf; border-radius:1px;\"></div>\n",
    "\n",
    "  <!-- Step 3 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#004b98; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 3: Merge weather data and social data\n",
    "  </div>\n",
    "  <div style=\"width:2px; height:28px; background:#bfbfbf; border-radius:1px;\"></div>\n",
    "\n",
    "  <!-- Step 4 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#003087; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 4: Visualize and analyze the combined dataset\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88370d06-a6aa-47e8-bb4a-cb4bb3f3b9d9",
   "metadata": {},
   "source": [
    "## Stop!  Think About Your Own Data \n",
    "\n",
    "Thinking about your data, what types of weather-related data might bring additional insights?  What are some questions that you're interested in? \n",
    "#### **>>Respond in the chat<<**\n",
    "\n",
    "<h2><span style=\"color:red\">Our Research Question Today</span></h2>\n",
    "\n",
    "For the purposes of this tutorial, we will explore the following question: does exposure to watches, warnings, and advisories have an impact on survey responses to weather risk perception.  Eventually, we will focus on flood warnings, watches, and advisories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867feb-2ffc-4d5e-9e14-3d4f277525e9",
   "metadata": {},
   "source": [
    "### Before We Get Started - Workplace Setup (Imports)\n",
    "\n",
    "In Jupyter, there are a large number of python-based \"libraries\" or \"packages\" that help with data loading, transformation, and analysis.  These libraries provide tools that help us do things like make graphs, work with data, or do more complex calculations, like regression.  It's good practice to have all libraries imported at the beginning.  You can always add (even install) libraries as you go along, you just have to rerun the cells (or restart the *kernel* if a new install).  \n",
    "\n",
    "To run a cell, you can go to **\"Run\"** in the Jupyter menu and select **\"Run selected cell\"**.  However, it is easier to click on the chevron (▶️) in the editing menu.  There are also keyboard shortcuts like **Shift+Enter** or **Ctrl+Enter**.\n",
    "\n",
    "In this tutorial, we are going to use a variety of libraries that can be grouped in the following categories:\n",
    "\n",
    "*Data Handling*\n",
    "\n",
    "- **pandas**: For working with tabular data in DataFrames.  It is commonly imported with an alias (pd), so we don't constantly have to type out pandas.\n",
    "- **requests**: For easily fetching data from web APIs and URLs (using GET, POST, etc).\n",
    "- **timedelta**: Imported from the datetime library, for representing time intervals.\n",
    "- **BytesIO**: Imported from the IO library, for treating in-memory bytes like a file for reading.\n",
    "- **ast**: A python module which can be used for evaluating strings.\n",
    "\n",
    "*Geospatial*\n",
    "\n",
    "- **geopandas**: To work with geospatial data, allowing us to perform spatial operations and handle geometries such as points, polygons, and lines.\n",
    "- **Point**: Imported from Shapely, for creating geometric points for mapping.\n",
    "\n",
    "*Visualization*\n",
    "\n",
    "- **Pyplot**: Imported from MatPlotLib using the alias \"plt\", for making simple customizable  plots and charts.\n",
    "- **Seaborn**: Imported using the alias \"sns\", for making better looking visualizations.\n",
    "  \n",
    "*Utilities*\n",
    "\n",
    "- **time**: Provides functions for working with time, such as measuring durations, pausing executions, and accessing system time.\n",
    "- **tqdm**: Adds progress bars to loops for tracking execution.\n",
    "- **IPython.display**: Renders python results in HTML\n",
    "\n",
    "*Statistical Modeling/Inference*\n",
    "\n",
    "- **scipy.stats.chi2_contingency**: Imported from Scipy, to run a chi-square test of independence to check if two categorical variables are related.\n",
    "- **statsmodels.api as sm**: Imported with the alias \"sm\", it provides tools for statistical models, including logistic regression.\n",
    "- **OrderedModel**: Imported from statsmodels, used for ordinal logistic regression models when outcomes are ordered categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691dcae-b33c-4ab8-917d-da4b7f2de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#utilities\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#statistical modeling/inference\n",
    "from scipy.stats import chi2_contingency        \n",
    "import statsmodels.api as sm                    \n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdde11",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0085ca66; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 1\n",
    "</div>\n",
    "\n",
    "## Step 1: Use an API to explore a repository, view metadata, and download data\n",
    "\n",
    "In this step, we will explore API access to a data repository, the Harvard Dataverse.  An API (Application Programming Interface) is just a set of rules and tools that allows different software and servers to communicate and interact with each other.  In this case, we want our Jupyter notebook to interact with the Harvard Databerse server for information on datasets.  We use the python library \"Requests\" to simplify and automate our requests, and Dataverse returns what we requested (hopefully), typically in a format called JSON.  We then use Pandas to transform the JSON in a dataframe, making the results easier to read and manipulate.  \n",
    "\n",
    "- **Note**: Not all APIs are the same and there might be differences across repositories and data servers.  Be sure to check each API's documentation for how to get started, authentication, search and data access, and more.  For Harvard's Dataverse, the [Dataverse API Guide](https://guides.dataverse.org/en/latest/api/index.html) is a comprehensive, up-to-date documentation for all operations in Harvard’s Dataverse.\n",
    "\n",
    "- **Another Note**: Ensure that the API is open access, or whether you need an **API key** for restricted access.  An API key is a unique code generated by the data provider that allows users to authenticate and access the data.  For this tutorial, the APIs do not require an API key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b78722-de05-4332-bd49-265d741a5cc0",
   "metadata": {},
   "source": [
    "#### Simple search for 10 results\n",
    "\n",
    "By default, the Harvard Dataverse only returns 10 results per search request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037550f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search query\n",
    "query = \"ripberger\"\n",
    "search_url = f\"https://dataverse.harvard.edu/api/search?q={query}&type=dataset\"\n",
    "\n",
    "# Perform search and show JSON\n",
    "response = requests.get(search_url)\n",
    "results = response.json()\n",
    "results  # Display raw JSON output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff161b94-d6cc-438c-a268-cada286c620f",
   "metadata": {},
   "source": [
    "#### Transform the JSON results\n",
    "\n",
    "We can easily transform the JSON results into a more readable format, a Pandas dataframe.  This bit of code we can keep separate or build into any cells where the results are returned in JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17b5b2-66e8-4080-988d-b0722fdffc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract items and load into DataFrame\n",
    "items = results['data']['items']\n",
    "df_results = pd.DataFrame(items)\n",
    "\n",
    "# Preview first 5 rows\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceea727-7c35-4052-859d-ec12e5d49a63",
   "metadata": {},
   "source": [
    "#### Let's examine that 'description' a little more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7150694-ffb5-42fc-b634-3694b4d4b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".dataframe td {\n",
    "  white-space: normal !important;\n",
    "  word-wrap: break-word;\n",
    "  max-width: 400px;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "df_results[[\"name\", \"description\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cf11b-d272-4823-bb22-001368af08b2",
   "metadata": {},
   "source": [
    "#### Search for more than 10 results\n",
    "\n",
    "We can make an API call that goes beyond the 10 result limit by creating a **loop**.  The \"while True\" statement will continue running (10 results at a time) until there are no results remaining.  We collect all of the results in one list using the \"extend\" command.  Finally, we can limit the dataframe to only view a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba6ce1-293f-4dd2-90a5-57cf01e33690",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ripberger\"\n",
    "start = 0\n",
    "per_page = 20  # Max per page is 100\n",
    "all_items = []\n",
    "\n",
    "while True:\n",
    "    search_url = (\n",
    "        f\"https://dataverse.harvard.edu/api/search?\"\n",
    "        f\"q={query}&type=dataset&start={start}&per_page={per_page}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    items = data['data']['items']\n",
    "    all_items.extend(items)\n",
    "    \n",
    "    # Break if fewer than per_page results are returned (i.e., last page)\n",
    "    if len(items) < per_page:\n",
    "        break\n",
    "    start += per_page\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(all_items)\n",
    "df_results[['name', 'global_id', 'published_at', 'citation']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cf708-914f-48b5-b581-a65ad628c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many datasets?  Each row (first number) represents a dataset.\n",
    "df_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3ced3-5580-48d3-bfbd-10617e4532db",
   "metadata": {},
   "source": [
    "### Subsetting Our Results\n",
    "\n",
    "Let's say we don't want all of these, but only a subset of related surveys.  For this step, we will focus on the yearly waves of the Extreme Weather and Society Survey (WXYY). So let's subset them the dataframe from the earlier API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0f9d5-63bc-4892-8a4f-88bf2f8550bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset names you want to filter on\n",
    "target_names = [\"WX17\", \"WX18\", \"WX19\", \"WX20\", \"WX21\", \"WX22\", \"WX23\", \"WX24\"]\n",
    "\n",
    "# Subset the dataframe\n",
    "df_subset = df_results[df_results['name'].isin(target_names)]\n",
    "\n",
    "#Reset colwidth\n",
    "pd.reset_option(\"display.max_colwidth\")\n",
    "\n",
    "# Display the result\n",
    "df_subset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb964d7",
   "metadata": {},
   "source": [
    "## Step 1a: Get Dataset Metadata and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584267c4-9f99-4f97-8881-1c095a6b8f5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Getting metadata for a single dataset\n",
    "\n",
    "Let's examine the metadata for one of the datasets in the subset, WX18.  There will often be two sets of metadata: file-level (specific to the repository) and data-level (that describes the data---the topic of yesterday's forum).  In this example, we will first look at the file-level metadata that the Dataverse uses.  Next, we will pull the full dataset metadata.  While the results will be in JSON, we will be adding a step that converts them to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85380a77-04f2-469a-b550-e34c16b25259",
   "metadata": {},
   "source": [
    "### File level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row [0] by position\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get dataset metadata\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Get list of files\n",
    "files = metadata_response['data']['latestVersion']['files']\n",
    "\n",
    "# Convert list of files to DataFrame\n",
    "df_files = pd.DataFrame(files)\n",
    "\n",
    "# Preview first few rows\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d8ce9-6a2d-4cc8-a053-5e10f8a04a0e",
   "metadata": {},
   "source": [
    "### Full dataset metadata\n",
    "\n",
    "This is actually a case of where we want to see the full JSON, to view those aspects of the dataset that are most useful.  In this case, the most relevent information can be found in a description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f4f8b-9f6a-4485-955c-f81dde9bc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get full dataset metadata (latest version)\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/versions/:latest?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Extract list of files from the JSON\n",
    "files = metadata_response['data']['files']\n",
    "\n",
    "metadata_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc278427",
   "metadata": {},
   "source": [
    "## Step 1b: Download a File\n",
    "\n",
    "From the file-level metadata, we see that the dataset file (.tab) is accompanied by PDFs of the instrument and a reference report.\n",
    "\n",
    "Let's download the dataset file (.tab) for first year of the survey, WX18, which has a file_id of \"3657710\".  If you wanted to work with this data for real, you would also want to download the survey instrument and reference document to fully understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a111b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File ID for WxEM_Wave1.tab\n",
    "file_id = 3657710\n",
    "\n",
    "# API call and download directly to memory\n",
    "file_url = f\"https://dataverse.harvard.edu/api/access/datafile/{file_id}?format=original\"\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# Load into pandas directly from memory, assuming comma-delimited content\n",
    "df_18 = pd.read_csv(BytesIO(response.content), sep=',', encoding='ISO-8859-1', engine='python', on_bad_lines='skip')\n",
    "df_18.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d029e2f-c5d5-4a96-b137-8a8d77ba7078",
   "metadata": {},
   "source": [
    "### Examine the dataset\n",
    "\n",
    "Pandas has a number of attributes that can be used to quickly examine characteristics of the dataset, things like size, shape, value distributions, basic stats, and missing values...anything you would normally do to check a dataset.  Below, I am just going to run a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b165-638f-4f8f-86c7-5baaf39002bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See basic shape of the data (rows, columns).  Each row represents a respondent.\n",
    "\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3b3ac-4b4e-4d62-a11c-05f1c46972b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See quick summary statistics\n",
    "\n",
    "df_18.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c13f3-573f-44f9-87dd-69db3e4b741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a quick count of missing data by column\n",
    "\n",
    "df_18.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada7a0f-3378-4f42-a588-383ac6530e3b",
   "metadata": {},
   "source": [
    "#### Variable names\n",
    "\n",
    "Next, we will list the columns to show all the variable names contained in the data.  To integrate with weather data, we are most interested in locating possible ways to join the data.  Typically, geographic variables are a good start. \n",
    "\n",
    "<h2><span style=\"color:red\">Stop!  After we run the next cell, do you see any geographic variables that might be useful?</span></h2>\n",
    "\n",
    "#### **>>Respond in the chat<<**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0012e2-7334-492d-8dd6-9026a265fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all column names\n",
    "df_18.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5994548-d663-41a2-bd1e-a3922742595a",
   "metadata": {},
   "source": [
    "In this survey, some good possible variables are state, zip, and lat/lon. These are pretty straightforward, but I'm also curious about \"nws_region\"? What does it contain? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9502-1d22-4520-a342-1035a7c5b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is nws_region usefl at all?\n",
    "df_18['nws_region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79825058-2652-4f7e-a37e-d2f3e71494a7",
   "metadata": {},
   "source": [
    "##### Unfortunately, \"nws_region\" only has four regions.  Nonetheless, it might be useful for a different research question.  \n",
    "\n",
    "### For our purposes, we will focus 'lat'/'lon' for location.  And we'll use begin_date to identify three days prior to beginning the survey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d675f60",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0069afff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 2\n",
    "</div>\n",
    "\n",
    "## Step 2: Use an API to download weather data\n",
    "\n",
    "First, we will demonstrate how the Iowa Mesonet API can be used to collect weather alerts for each survey respondent.  Since calling an API can be slow and depends on internet access, we have already pre-downloaded the weather data needed for this analysis.  \n",
    "\n",
    "Next, we will take this pre-loaded weather data and merge it with the survey responses so that each person’s record includes both their answers and the relevant weather alerts.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982baaf3-b0e5-4923-9af9-dc5cdf42695b",
   "metadata": {},
   "source": [
    "### Iowa Mesonet API - DO NOT RUN AT THIS TIME\n",
    "\n",
    "The code below is doing alot of work, going through the survey data **one person/row at a time** and asking the Iowa Mesonet API:  \n",
    "\n",
    "“Given this person’s location and survey date, what watches, warnings, or advisories (WWAs) were active in the few days leading up to that date?”  \n",
    "\n",
    "**Analogy:** This is like calling a weather hotline for each person’s hometown and writing down any recent alerts next to their name in the survey spreadsheet.  \n",
    "\n",
    "Here’s what happens step by step:  \n",
    "\n",
    "1. **Make sure the date is in the right format.**  \n",
    "   The `begin_date` column is converted into a standard date format so the computer can work with it (it's a precaution). \n",
    "\n",
    "2. **Prepare a place to store the results.**  \n",
    "   A new column called `wwa_names` is added to the survey data. This will eventually hold a *list* of weather alerts for each person.  In Python, you can think of a list as a flexible container that holds a collection of items.\n",
    "\n",
    "3. **Go through each respondent one by one.**  \n",
    "   For each person, we:  \n",
    "   - Look up their latitude, longitude, and survey date.  \n",
    "   - Define a **3-day window** before their survey date (so we catch recent alerts).  \n",
    "   - Build a request to the Iowa Mesonet API using their location and dates.  \n",
    "\n",
    "\n",
    "4. **Ask the API for weather alerts.**  \n",
    "   - If the API responds successfully, we pull out the names of any alerts and save them in that person’s row.  \n",
    "   - If something goes wrong (bad connection, no data, etc.), we save an *empty list* for that person.\n",
    "   - The API call will continue until we run out of survey respondents.  \n",
    "\n",
    "The resulting dataset (`df_18`) has a new column called `wwa_names` that tells us which WWAs (if any) each respondent experienced around the time of their survey.  \n",
    "\n",
    "Please note the API Endpoint (Rest-like) and the structure of the request for those variables:\n",
    "\n",
    "https://mesonet.agron.iastate.edu/vtec/json.php?lon={lon}&lat={lat}&sdate={start}&edate={end}\n",
    "\n",
    "Remember, you can always check the API documentation for guidance: the [Iowa Mesonet API Guide](https://mesonet.agron.iastate.edu/api/).\n",
    "\n",
    "**Note:  We will not run this API call during the webinar because it takes about 20-30 minutes to download the data. If you want to try this after the webinar, just remove the first line ('''python) and run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03230106-daee-42e8-99d1-8e635f846478",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''python\n",
    "\n",
    "# Make sure begin_date is in datetime format\n",
    "df_18['begin_date'] = pd.to_datetime(df_18['begin_date'])\n",
    "\n",
    "# New column to store list of WWA names\n",
    "df_18['wwa_names'] = None\n",
    "\n",
    "# Loop through each respondent\n",
    "for idx, row in tqdm(df_18.iterrows(), total=len(df_18)):\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    end_date = row['begin_date']\n",
    "    start_date = end_date - timedelta(days=3)\n",
    "\n",
    "    # Build API URL with small buffer\n",
    "    url = (\n",
    "        f\"https://mesonet.agron.iastate.edu/json/vtec_events_bypoint.py\"\n",
    "        f\"?lat={lat}&lon={lon}\"\n",
    "        f\"&sdate={start_date.strftime('%Y-%m-%d')}&edate={end_date.strftime('%Y-%m-%d')}\"\n",
    "        f\"&buffer=0.1\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            names = [event['name'] for event in data.get('events', [])]\n",
    "            df_18.at[idx, 'wwa_names'] = names\n",
    "        else:\n",
    "            df_18.at[idx, 'wwa_names'] = []\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for idx={idx}, lat={lat}, lon={lon}: {e}\")\n",
    "        df_18.at[idx, 'wwa_names'] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776736c5-9b2e-42e3-9f4d-91fea28a6d37",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#004b98ff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 3\n",
    "</div>\n",
    "\n",
    "\n",
    "### Step 3 - Joining the Survey Data with Weather Alerts\n",
    "\n",
    "Prior to the webinar, the data we needed was downloaded in a csv file and stored in Github.  This dataset ('wwa_by_pid') includes the respondent identifier ('p_id') and a column with watches, warnings, and advisories ('wwa_names') in a list.  At this point, we have two different tables of data that we need to merge:\n",
    "\n",
    "- **Survey data** (df_18): this has all the survey responses, including their p_id (a unique identifier for each person).\n",
    "\n",
    "- **Weather alerts data** (lk): this has just two columns — the same p_id, and the list of watches, warnings, or advisories (wwa_names) that each person experienced.\n",
    "\n",
    "We're going to do a left merge on both tables using p_id, which is basically doing the following: \n",
    "\n",
    "**“For each person (p_id) in the survey data, look up their matching p_id in the weather file and bring in the weather alerts column (wwa_names).”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6aaa1f-d227-45e6-afc8-bf0cd0c798e6",
   "metadata": {},
   "source": [
    "##### If you ran the API call in Step 2, you should skip the next two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0489d-353f-497c-b1cc-2292f5b61e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have run the API, you should skip this cell\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jmote-noaa/Data-Forums/refs/heads/main/data/wwa_by_pid.csv\"\n",
    "lk = pd.read_csv(url)\n",
    "\n",
    "# Merge with the original df_18 on 'p_id'\n",
    "df_18 = df_18.merge(lk, on=\"p_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5b7b4-93ba-4c44-ad61-73f68d0d2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use `ast` to convert the `wwa_names` column from text back into real Python lists so we can work with them.  \n",
    "df_18['wwa_names'] = df_18['wwa_names'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b52e5a-dc51-4fda-9531-51307bd8c0e8",
   "metadata": {},
   "source": [
    "##### If you ran the API call in Step 2, you can resume here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69d50-18ea-496e-9ffa-3e982f319211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the new, combined dataset\n",
    "df_18.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba1ca0-c11e-40cb-8fe0-939eea5e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can subset the dataframe to focus only on the variables we want to see.  We use the attribute \"dropna\" to make sure that there are no rows that are empty (i.e., we didn't screw something up).\n",
    "df_18[['p_id','lat', 'lon', 'begin_date', 'wwa_names']].dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfd706-64e8-4b5d-8fe8-c1f7c136723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make sure we have the same number of row that we started with.  It should be 3,000\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479299b-22fb-4e6b-9968-58ce1f5ac62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many respondents experienced watches/warnings/advisories?\n",
    "df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fecd87-078a-43f9-88ff-54b947ac89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#who received more than two?\n",
    "df_18[df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 2)\n",
    "][['p_id','lat', 'lon', 'begin_date', 'wwa_names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1bd8-26cb-446d-a4e7-900ee14f6e40",
   "metadata": {},
   "source": [
    "### Step 3a: Exporting the Data for Use Outside Jupyter\n",
    "\n",
    "Now that we have a merged dataframe (df_18) containing both survey responses and weather alerts, we can save and download it in different formats for use in other tools and workflows. Jupyter is excellent for exploring, cleaning, and manipulating data, but once you’ve shaped the dataset you want, you may prefer to bring it back into your regular workflow — whether that’s statistical software, spreadsheets, or visualization platforms. We won’t actually run the save commands here, but I’ll provide the code snippets so you can see how it’s done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8f528-2648-4ce9-b358-46ee2b7ebe43",
   "metadata": {},
   "source": [
    "**CSV (general use--Excel, R, SAS, SPSS, STATA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd8a1d-590c-4efa-8bcd-da4c5fb3a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_18.to_csv(\"survey_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00482200-ba23-48de-aba7-00f5eabfbe04",
   "metadata": {},
   "source": [
    "**Parquet (general use--generally for larger datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b334d-b553-4aa9-b63a-fb6c549f71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_18.to_parquet(\"survey_weather.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c041028-8287-4a2d-b4c8-7710da2d1330",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fb1e1eff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Stop!  Take a break! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead4b5-ceb4-4e74-8ab1-bfa05ecd9034",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#003087ff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 4\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Exploring the Combined Dataset\n",
    "\n",
    "After you download the new dataset, you could easily just jump back into your regular workflow.  But let's say we want to continue in Jupyter to quickly visualize and analyze the data to examine the data for any insights.  Below we will look at the following:\n",
    "\n",
    "- ***Quick Visualizations***\n",
    "- ***Visualizing WWAs by Survey Responses***\n",
    "- ***Quick Statistical Analyses***\n",
    "- ***A Bit More Involved Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d2172-d491-44ee-8e14-53844f93ce9f",
   "metadata": {},
   "source": [
    "#### Step 4a: Quick visualizations\n",
    "\n",
    "In this step, I'll go over the first visualization and show the code for the remaining two:\n",
    "\n",
    "1. A geomap of survey respondents by number of watches and warnings 3 days prior to the survey.\n",
    "2. A bar chart showing frequency of types of watches and warnings.\n",
    "3. A line graph showing frequency of types of watches and warning over time.\n",
    "\n",
    "Jupyter (and Python) provide access to a wide range of powerful visualization libraries — from simple plotting with **matplotlib** and **seaborn** to interactive mapping with **folium** or **plotly** — giving you many options for exploring and presenting your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3200f-03e3-4ccd-ba8d-1ee1f1c5e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure lat/lon are float (just in case)\n",
    "df_18['lat'] = pd.to_numeric(df_18['lat'], errors='coerce')\n",
    "df_18['lon'] = pd.to_numeric(df_18['lon'], errors='coerce')\n",
    "\n",
    "# Step 2: Create geometry from lat/lon\n",
    "geometry = [Point(xy) for xy in zip(df_18['lon'], df_18['lat'])]\n",
    "gdf_18 = gpd.GeoDataFrame(df_18, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Step 3: Count WWAs\n",
    "gdf_18['wwa_count'] = gdf_18['wwa_names'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Step 4: Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "gdf_18.plot(column='wwa_count', cmap='plasma_r', legend=True, ax=ax, markersize=10)\n",
    "ax.set_title(\"Survey Respondents by Number of WWAs (3 Days Prior)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb67ea-8ce5-4ce5-911f-2fa3f0dee664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Flatten and count\n",
    "all_names = df_18['wwa_names'].dropna().explode()\n",
    "top_names = Counter(all_names).most_common(10)\n",
    "\n",
    "# Plot\n",
    "names, counts = zip(*top_names)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(names[::-1], counts[::-1])\n",
    "plt.title(\"Top 10 Most Frequent WWAs\")\n",
    "plt.xlabel(\"Number of Respondents Exposed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b5893-7286-410b-a6c3-594d62c40043",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' python\n",
    "\n",
    "# Count WWAs per date\n",
    "timeline = (\n",
    "    df_18[['begin_date', 'wwa_names']]\n",
    "    .dropna()\n",
    "    .assign(wwa_count=lambda df: df['wwa_names'].apply(len))\n",
    "    .groupby('begin_date')['wwa_count']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Plot\n",
    "timeline.plot(marker='o', figsize=(10, 4), title='Total WWAs by Survey Date')\n",
    "plt.ylabel('Total Warnings/Watch Events')\n",
    "plt.xlabel('Survey Date')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c6a06-2fd5-48eb-bb08-46818c8aaa24",
   "metadata": {},
   "source": [
    "### Step 4b: Visualizing WWWAs by Survey Reponses\n",
    "\n",
    "For a next step, we might undertake a visualization that is a bit more involved.\n",
    "\n",
    "Let's say we want to examine how the presence of recent weather alerts (3 days prior to taking the survey) correlates with survey respondents' perceived risk of that hazard.  Again, we're going to focus on flood-related WWAs.  How can we do that?  \n",
    "\n",
    "Step 1.  Create two groups of respondents, those experienced a WWA prior to the survey and those who did not.\n",
    "Step 2.  Extract risk perception scores for a hazard (For risk_flood: 1-No risk, 2-Low Risk.....5-Extreme risk)\n",
    "Step 3.  Create comparative visualizations\n",
    "\n",
    "For the first two steps, we have created a function **(def)** to carry out those tasks.  We could easily do this without creating a function but it would take several steps to do so.  Here we get it done in one cell.  When you run this cell, you will notice that there are no results...we're just setting things up for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211e79f-df3e-4b56-9d2d-83c13af46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prep: robust WWA exposure + ordered risk labels ---\n",
    "\n",
    "# 1) robust flood_wwa_exposure (handles NaN/non-lists)\n",
    "flood_wwa_keywords = ['Flood Advisory', 'Flood Warning', 'Flash Flood Warning', 'Flash Flood Watch', 'Flood Watch']\n",
    "\n",
    "def has_flood_wwa(wwas):\n",
    "    if isinstance(wwas, (list, tuple, set)):\n",
    "        return any(k in wwas for k in flood_wwa_keywords)\n",
    "    return False\n",
    "\n",
    "df_18['flood_wwa_exposure'] = df_18['wwa_names'].apply(has_flood_wwa)\n",
    "\n",
    "# 2) numeric -> labeled flood risk (ordered categorical)\n",
    "label_map = {1: 'No risk', 2: 'Low risk', 3: 'Moderate risk', 4: 'High risk', 5: 'Extreme risk'}\n",
    "ordered_risk_labels = list(label_map.values())\n",
    "\n",
    "# Coerce to numeric, map to labels, and set categorical order\n",
    "df_18['risk_flood_num'] = pd.to_numeric(df_18.get('risk_flood'), errors='coerce')\n",
    "df_18['risk_flood_label'] = pd.Categorical(\n",
    "    df_18['risk_flood_num'].map(label_map),\n",
    "    categories=ordered_risk_labels,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "#Let's take a look at what we just did\n",
    "\n",
    "df_18[['wwa_names', 'flood_wwa_exposure', 'risk_flood_num', 'risk_flood_label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084549b3-555b-44c8-86af-c9c840ff4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualize what we just did\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(\n",
    "    data=df_18,\n",
    "    x='risk_flood_label',\n",
    "    hue='flood_wwa_exposure',\n",
    "    order=ordered_risk_labels,\n",
    "    hue_order=[False, True]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Perceived Flood Risk\")\n",
    "plt.ylabel(\"Number of Respondents\")\n",
    "plt.title(\"Perceived Flood Risk by Exposure to Flood-Related WWAs\")\n",
    "plt.legend(title=\"WWA Exposure\", labels=[\"No\", \"Yes\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e72242-80ba-4658-9c5c-b89c04c71d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' python\n",
    "\n",
    "# crosstab -> proportions by exposure\n",
    "crosstab = pd.crosstab(\n",
    "    df_18['flood_wwa_exposure'],\n",
    "    df_18['risk_flood_label'],\n",
    "    normalize='index'\n",
    ")\n",
    "\n",
    "# ensure consistent order of rows/columns even if some levels are missing\n",
    "crosstab = crosstab.reindex(index=[False, True], columns=ordered_risk_labels)\n",
    "\n",
    "ax = crosstab.T.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "\n",
    "plt.title('Flood Risk Perception by WWA Exposure (Proportions)')\n",
    "plt.xlabel('Perceived Flood Risk')\n",
    "plt.ylabel('Proportion of Respondents')\n",
    "plt.legend(title='Exposed to Flood WWA', labels=['No', 'Yes'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257784d-91b7-495d-b2a7-87e6b8a29518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots are interesting, let's take a look.  It looks a bit like a population pyramid.\n",
    "sns.violinplot(data=df_18, x='flood_wwa_exposure', y='risk_flood', inner='quartile')\n",
    "plt.xticks([0, 1], ['No WWA Exposure', 'WWA Exposure'])\n",
    "plt.xlabel(\"Exposure to Flood WWA\")\n",
    "plt.ylabel(\"Perceived Flood Risk (1-5)\")\n",
    "plt.title(\"Distribution of Flood Risk Perception by WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b31535-6c89-4c43-935b-f1d5b7934c54",
   "metadata": {},
   "source": [
    "## Step 4C: Quick Statistical Analyses\n",
    "\n",
    "We're not limited to visualizations, we can explore a number of statistical procedures to begin to test our question of whether exposure to warnings and watches has an impact on survey responses.  For this tutorial, we will run the following test:\n",
    "\n",
    "1. Chi-Square: To evaluate whether exposure to flood-related warnings and watches and perceived flood risk categories are independent, with the p-value indicating if the observed relationship is statistically significant.\n",
    "\n",
    "I will provide the code for two additional analysis, so you can see how they are set up.  If you want to run them on your own, just remove the first line (''' python) or put a hash (#) in front of it (#'''python).\n",
    "\n",
    "1. Logistic Regression: To evaluate whether exposure to flood-related warnings and watches increases the odds of respondents reporting high or extreme flood risk compared to lower risk levels.\n",
    "2. Ordinal Logistic Regression: To evaluate whether exposure to flood-related warnings and watches shifts respondents toward higher categories of perceived flood risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48102616-f3ad-47b9-a863-48aff2a5efd3",
   "metadata": {},
   "source": [
    "### 1. Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9805ec1-577e-4e29-bf8b-c58571dd9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a contingency table\n",
    "\n",
    "contingency = pd.crosstab(df_18['flood_wwa_exposure'], df_18['risk_flood_label'])\n",
    "contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5af11-fe0f-42ff-bad1-45e6aa494002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test it\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\"Chi-square test\")\n",
    "print(f\"Chi2 = {chi2:.2f}, df = {dof}, p = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d836569-37c4-497e-9939-431d6ac8fdc0",
   "metadata": {},
   "source": [
    "This suggests that the distribution of flood risk perceptions is not independent of WWA exposure — in other words, people who were exposed to flood-related weather alerts responded differently (in terms of risk levels) than those who were not exposed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3943fc-95ab-4b32-ace2-8a0f376bf859",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression\n",
    "\n",
    "We're going to fit a logistic regression model to test whether exposure to a flood alerts predicts whether a survey respondent reports being at “high” or “extreme” flood risk.\n",
    "\n",
    "In short: the regression asks, “How much more likely are people to report high or extreme flood risk if they were exposed to a flood-related alerts?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60e3b-5a33-4a9f-9d0f-d82366e42625",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''python\n",
    "\n",
    "# Binary outcome: high risk (≥4) vs. lower\n",
    "df_18['high_risk'] = df_18['risk_flood_num'] >= 4\n",
    "\n",
    "# Predictor: WWA exposure (cast to int)\n",
    "X = sm.add_constant(df_18['flood_wwa_exposure'].astype(int))\n",
    "y = df_18['high_risk'].astype(int)\n",
    "\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2c04a-7e01-4624-aea1-e55ed519613c",
   "metadata": {},
   "source": [
    "This suggests that respondents exposed to a flood WWA had significantly higher odds (about 39% greater, exp(0.3326) ≈ 1.39) of reporting high or extreme flood risk compared to those not exposed, though the overall model explains only a small share of variation in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c6f48-4a63-4943-8ee3-71812abf60ba",
   "metadata": {},
   "source": [
    "### 3. Ordinal Logistic Regression\n",
    "\n",
    "The previous regression was a binary logistic regression, and the outcome was simplified into two categories: high risk (4–5) vs. not high (1–3).  The model only asks: “Does exposure change the odds of being high risk or not?”\n",
    "\n",
    "With this model, we use the full ordered scale (1-5), which accounts for the fact that reporting risk = 5 is “higher” than risk = 4, which is higher than risk = 3, etc.\n",
    "\n",
    "Instead of focusing on a single cutoff, the model estimates the effect of exposure across all possible thresholds in the risk scale.  And you get cut-points plus one slope coefficient. The slope tells you whether exposure consistently increases the likelihood of reporting any higher category of risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4dc504-1897-44b6-bf0b-5222f52dae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs to avoid issues\n",
    "df_ord = df_18.dropna(subset=['risk_flood_num', 'flood_wwa_exposure'])\n",
    "\n",
    "# Predictor must be numeric (int)\n",
    "X = df_ord[['flood_wwa_exposure']].astype(int)\n",
    "\n",
    "# Outcome is ordered categories (numeric risk levels already ordered 1–5)\n",
    "y = df_ord['risk_flood_num']\n",
    "\n",
    "# Fit ordinal logistic regression\n",
    "mod = OrderedModel(y, X, distr='logit')\n",
    "res = mod.fit(method='bfgs')\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262f9dd-e592-410f-9d1b-9b053e8c4004",
   "metadata": {},
   "source": [
    "This suggests that respondents exposed to flood-related warnings and watches had a significantly higher likelihood of placing themselves in higher flood risk perception categories (coef = 0.2810, p < 0.000), indicating a consistent upward shift in perceived risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6a768-069c-4eda-8c8f-0bcdee2245de",
   "metadata": {},
   "source": [
    "## A Bit More Involved - Did WWAs *Really* Affect Risk Perception?\n",
    "\n",
    "So it appears that exposure to watches and warnings has a statistically significant impact on survey responses.  But by how much (substantive significance)? To suggest an answer, we might look at the **predicted probabilities** of survey responses.  This shows us how much exposure to a WWA changes the likelihood of a respondent reporting “High” or “Extreme” flood risk.  \n",
    "\n",
    "To do this, the code below creates two simple scenarios: one where a person had no flood alert (0) and one where they did (1).  It then uses a statistical model (ordinal logit model with a logit link function) to predict the probability of each risk category under those two scenarios.  The results are labeled clearly as “No Exposure” and “Exposure,” giving us a side-by-side view.  \n",
    "\n",
    "**Analogy:** It’s like asking, “What would the risk look like if nobody had a flood alert?” and then,  “What would the risk look like if everyone had a flood alert?” — and comparing the two answers side by side.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3bdae-4502-4d07-8e19-5389cbb76050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two scenarios: no exposure (0) and exposure (1)\n",
    "scenarios = pd.DataFrame({\n",
    "    'flood_wwa_exposure': [0, 1]\n",
    "})\n",
    "\n",
    "# Predict probabilities for each risk category\n",
    "pred_probs = res.predict(scenarios)\n",
    "\n",
    "# Attach labels\n",
    "pred_probs.index = ['No Exposure', 'Exposure']\n",
    "pred_probs.columns = [f\"Risk {c}\" for c in pred_probs.columns]\n",
    "\n",
    "pred_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a547651-6f6e-42a7-9d62-5716cc99e669",
   "metadata": {},
   "source": [
    "## A possible interpretation\n",
    "\n",
    "Respondents who were exposed to a flood WWA were less likely to report no risk (10% → 8%) or low risk (32% → 28%), and more likely to report higher levels of risk perception, particularly at the “High” (15% → 18%) and “Extreme” (9% → 11%) categories. While the percentage point changes may look modest, they indicate a clear upward shift in perceived flood risk among those who received WWAs.  In other words, I think it is possible to say that exposure to a flood warning nudged people away from saying ‘no risk’ and toward saying ‘high or extreme risk.  However, with only 3,000 responses, I probably wouldn't.  As the academics say, this requires further study. 😉\n",
    "\n",
    "Now let's take a look at a visualization of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d88ad-5778-4c2a-8c1f-88a3a3da1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see HOW the probability distribution shift\n",
    "ax = pred_probs.T.plot(kind='bar', figsize=(10,6))\n",
    "plt.title(\"Predicted Flood Risk Perception by WWA Exposure\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Perceived Flood Risk Level\")\n",
    "plt.legend(title=\"WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e86aa-c61b-4c40-ad08-13ec49961f36",
   "metadata": {},
   "source": [
    "## Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5570c5-de5c-4d01-8741-1fb8b8f7e3ba",
   "metadata": {},
   "source": [
    "In this tutorial, we explored how to bring together **social survey data** and **weather warning data** to better understand how hazard information might influence perceptions of risk. Using the Jupyter Notebook environment, you saw how to:  \n",
    "\n",
    "- **Work with APIs** to search for, access, and download data programmatically  \n",
    "- Organize and explore datasets interactively using python libraries like **pandas** and **requests**.  \n",
    "- Merge survey data with external data from the **Iowa Environmental Mesonet**  \n",
    "- Apply **geospatial tools** to handle location-based data  \n",
    "- Create clear, reproducible **visualizations** and **statistical analyses** directly alongside your analysis  \n",
    "- Document your process in a way that combines code, results, and explanation all in one place  \n",
    "\n",
    "With this walkthrough, you’ve seen how Jupyter can serve as both a **research lab and a communication tool**—a space where you can work with data, analyze the data, visualize the data and results, and explain what you found.  \n",
    "\n",
    "### Moving Forward  \n",
    "- Try adapting this workflow to other survey topics (e.g., heat, drought, tornado risk)  \n",
    "- Explore additional APIs to enrich your analysis with different kinds of data  \n",
    "- Use Jupyter notebooks to build **reproducible reports**, where readers can see not just your conclusions but also the steps you took to get there  \n",
    "- Share your notebooks with collaborators as a way to make your analysis **transparent and interactive** \n",
    "\n",
    "Ultimately, the key takeaway is that with just a few tools—**APIs, pandas, geospatial libraries, and Jupyter notebooks**—you can connect diverse datasets, analyze them in context, and tell meaningful data stories about risk and society **in one place**!\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for following along!**  \n",
    "We encourage you to take this workflow and apply it to your own research questions about weather, risk, and society—the more you explore, the more insights you’ll uncover.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
