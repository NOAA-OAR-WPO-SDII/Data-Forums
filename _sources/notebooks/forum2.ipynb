{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff320bf9",
   "metadata": {},
   "source": [
    "# Forum 2\n",
    "\n",
    "This notebook is designed for social scientists who want to explore how their own data might begin to interface with weather and hazard datasets.\n",
    "\n",
    "### Goals:\n",
    "1. Load and explore a sample of the Extreme Weather and Society Survey data\n",
    "2. Bring in a sample weather dataset (e.g., watches/warnings/advisories from NWS)\n",
    "3. Perform a simple merge\n",
    "4. Visualize a relationship between survey responses and weather\n",
    "\n",
    "Note: Ensure that the datasets are open access, or you have an API key for restricted access.  For this example, the datasets in this notebook do not require an API key.\n",
    "\n",
    "### Why do this?\n",
    "\n",
    "In general, the integration of weather and hazard data allows for a deeper understanding of how people perceive, interpret, and respond to weather and climate risks in relation to the actual meteorological conditions they experience.  A range of potential researchs can be posed:\n",
    "\n",
    "1. How accurately do people perceive the frequency or severity of extreme weather events in their area?\n",
    "2. Are there socioeconomic or demographic predictors of inaccurate weather risk perception?\n",
    "3. Do people who recall receiving more weather warnings perceive greater weather risk?\n",
    "4. Is there a mismatch between how meteorologists frame risk and how the public interprets it?\n",
    "5. Are individuals who have experienced extreme weather more likely to change their behavior or beliefs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867feb-2ffc-4d5e-9e14-3d4f277525e9",
   "metadata": {},
   "source": [
    "### Import \n",
    "\n",
    "In Python, we import libraries to add extra tools that help us do things like make graphs, work with data, or do more complext calculations, like regression.  It's good practice to have all libraries imported at the beginning.  You can always add (even install) libraries as you go along, you just have to rerun the cells (or restart the kernel if a new install)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2691dcae-b33c-4ab8-917d-da4b7f2de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import BytesIO\n",
    "from datetime import timedelta\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdde11",
   "metadata": {},
   "source": [
    "## Step 1: Search for Datasets\n",
    "\n",
    "By default, the Harvard Dataverse only returns 10 results per search request.  The first cell shows a search and retrieving only 10 results.  If you know there are more datasets available, the second cell shows to extend the search to retrieve all of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b78722-de05-4332-bd49-265d741a5cc0",
   "metadata": {},
   "source": [
    "##### Simple search for 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037550f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>global_id</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>publisher</th>\n",
       "      <th>citationHtml</th>\n",
       "      <th>identifier_of_dataverse</th>\n",
       "      <th>name_of_dataverse</th>\n",
       "      <th>...</th>\n",
       "      <th>minorVersion</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>contacts</th>\n",
       "      <th>authors</th>\n",
       "      <th>publications</th>\n",
       "      <th>image_url</th>\n",
       "      <th>keywords</th>\n",
       "      <th>producers</th>\n",
       "      <th>relatedMaterial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Climate Change Helplessness, Pilot Study</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/HAEP2K</td>\n",
       "      <td>doi:10.7910/DVN/HAEP2K</td>\n",
       "      <td>Replication data for Pilot Study for Climate C...</td>\n",
       "      <td>2015-12-12T21:20:58Z</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>Salomon, Erika, 2015, \"Climate Change Helpless...</td>\n",
       "      <td>cch</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-12T21:20:24Z</td>\n",
       "      <td>2015-12-12T21:20:58Z</td>\n",
       "      <td>[{'name': 'Salomon, Erika', 'affiliation': 'Un...</td>\n",
       "      <td>[Salomon, Erika]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate Change Helplessness, Study 1</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/OVZLG5</td>\n",
       "      <td>doi:10.7910/DVN/OVZLG5</td>\n",
       "      <td>Replication data for Study 1 of Climate Change...</td>\n",
       "      <td>2015-10-10T22:12:30Z</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>Salomon, Erika, 2015, \"Climate Change Helpless...</td>\n",
       "      <td>cch</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-06-26T20:32:33Z</td>\n",
       "      <td>2015-10-10T22:12:30Z</td>\n",
       "      <td>[{'name': 'Salomon, Erika', 'affiliation': 'Un...</td>\n",
       "      <td>[Salomon, Erika]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate Change Helplessness, Study 2</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/8Z7M4G</td>\n",
       "      <td>doi:10.7910/DVN/8Z7M4G</td>\n",
       "      <td>Replication data for Study 2 of Climate Change...</td>\n",
       "      <td>2015-10-10T22:13:10Z</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>Salomon, Erika, 2015, \"Climate Change Helpless...</td>\n",
       "      <td>cch</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-06-26T20:34:45Z</td>\n",
       "      <td>2015-10-10T22:13:10Z</td>\n",
       "      <td>[{'name': 'Salomon, Erika', 'affiliation': 'Un...</td>\n",
       "      <td>[Salomon, Erika]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Climate Change Helplessness, Study 3</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/KXW0LE</td>\n",
       "      <td>doi:10.7910/DVN/KXW0LE</td>\n",
       "      <td>Replication data for Study 2 of Climate Change...</td>\n",
       "      <td>2015-10-10T22:13:58Z</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>Salomon, Erika, 2015, \"Climate Change Helpless...</td>\n",
       "      <td>cch</td>\n",
       "      <td>Climate Change Helplessness Dataverse</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-06-26T20:42:33Z</td>\n",
       "      <td>2015-10-10T22:13:58Z</td>\n",
       "      <td>[{'name': 'Salomon, Erika', 'affiliation': 'Un...</td>\n",
       "      <td>[Salomon, Erika]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_QGIS file with island and climate change ris...</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/O6G3AI</td>\n",
       "      <td>doi:10.7910/DVN/O6G3AI</td>\n",
       "      <td>QGIS file to visualise and analyse climate cha...</td>\n",
       "      <td>2022-10-28T10:09:02Z</td>\n",
       "      <td>I Climate Change Risk Assessment</td>\n",
       "      <td>Lammers, Katrin; Gerbatsch, Karoline, 2022, \"3...</td>\n",
       "      <td>ClimRisk</td>\n",
       "      <td>I Climate Change Risk Assessment</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-10-24T14:38:34Z</td>\n",
       "      <td>2022-10-28T10:09:02Z</td>\n",
       "      <td>[{'name': 'Lammers, Katrin', 'affiliation': 'R...</td>\n",
       "      <td>[Lammers, Katrin, Gerbatsch, Karoline]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name     type  \\\n",
       "0           Climate Change Helplessness, Pilot Study  dataset   \n",
       "1               Climate Change Helplessness, Study 1  dataset   \n",
       "2               Climate Change Helplessness, Study 2  dataset   \n",
       "3               Climate Change Helplessness, Study 3  dataset   \n",
       "4  3_QGIS file with island and climate change ris...  dataset   \n",
       "\n",
       "                                  url               global_id  \\\n",
       "0  https://doi.org/10.7910/DVN/HAEP2K  doi:10.7910/DVN/HAEP2K   \n",
       "1  https://doi.org/10.7910/DVN/OVZLG5  doi:10.7910/DVN/OVZLG5   \n",
       "2  https://doi.org/10.7910/DVN/8Z7M4G  doi:10.7910/DVN/8Z7M4G   \n",
       "3  https://doi.org/10.7910/DVN/KXW0LE  doi:10.7910/DVN/KXW0LE   \n",
       "4  https://doi.org/10.7910/DVN/O6G3AI  doi:10.7910/DVN/O6G3AI   \n",
       "\n",
       "                                         description          published_at  \\\n",
       "0  Replication data for Pilot Study for Climate C...  2015-12-12T21:20:58Z   \n",
       "1  Replication data for Study 1 of Climate Change...  2015-10-10T22:12:30Z   \n",
       "2  Replication data for Study 2 of Climate Change...  2015-10-10T22:13:10Z   \n",
       "3  Replication data for Study 2 of Climate Change...  2015-10-10T22:13:58Z   \n",
       "4  QGIS file to visualise and analyse climate cha...  2022-10-28T10:09:02Z   \n",
       "\n",
       "                               publisher  \\\n",
       "0  Climate Change Helplessness Dataverse   \n",
       "1  Climate Change Helplessness Dataverse   \n",
       "2  Climate Change Helplessness Dataverse   \n",
       "3  Climate Change Helplessness Dataverse   \n",
       "4       I Climate Change Risk Assessment   \n",
       "\n",
       "                                        citationHtml identifier_of_dataverse  \\\n",
       "0  Salomon, Erika, 2015, \"Climate Change Helpless...                     cch   \n",
       "1  Salomon, Erika, 2015, \"Climate Change Helpless...                     cch   \n",
       "2  Salomon, Erika, 2015, \"Climate Change Helpless...                     cch   \n",
       "3  Salomon, Erika, 2015, \"Climate Change Helpless...                     cch   \n",
       "4  Lammers, Katrin; Gerbatsch, Karoline, 2022, \"3...                ClimRisk   \n",
       "\n",
       "                       name_of_dataverse  ... minorVersion  \\\n",
       "0  Climate Change Helplessness Dataverse  ...            0   \n",
       "1  Climate Change Helplessness Dataverse  ...            0   \n",
       "2  Climate Change Helplessness Dataverse  ...            0   \n",
       "3  Climate Change Helplessness Dataverse  ...            0   \n",
       "4       I Climate Change Risk Assessment  ...            0   \n",
       "\n",
       "              createdAt             updatedAt  \\\n",
       "0  2015-12-12T21:20:24Z  2015-12-12T21:20:58Z   \n",
       "1  2015-06-26T20:32:33Z  2015-10-10T22:12:30Z   \n",
       "2  2015-06-26T20:34:45Z  2015-10-10T22:13:10Z   \n",
       "3  2015-06-26T20:42:33Z  2015-10-10T22:13:58Z   \n",
       "4  2022-10-24T14:38:34Z  2022-10-28T10:09:02Z   \n",
       "\n",
       "                                            contacts  \\\n",
       "0  [{'name': 'Salomon, Erika', 'affiliation': 'Un...   \n",
       "1  [{'name': 'Salomon, Erika', 'affiliation': 'Un...   \n",
       "2  [{'name': 'Salomon, Erika', 'affiliation': 'Un...   \n",
       "3  [{'name': 'Salomon, Erika', 'affiliation': 'Un...   \n",
       "4  [{'name': 'Lammers, Katrin', 'affiliation': 'R...   \n",
       "\n",
       "                                  authors  publications image_url  keywords  \\\n",
       "0                        [Salomon, Erika]           NaN       NaN       NaN   \n",
       "1                        [Salomon, Erika]           NaN       NaN       NaN   \n",
       "2                        [Salomon, Erika]           NaN       NaN       NaN   \n",
       "3                        [Salomon, Erika]           NaN       NaN       NaN   \n",
       "4  [Lammers, Katrin, Gerbatsch, Karoline]          [{}]       NaN       NaN   \n",
       "\n",
       "   producers relatedMaterial  \n",
       "0        NaN             NaN  \n",
       "1        NaN             NaN  \n",
       "2        NaN             NaN  \n",
       "3        NaN             NaN  \n",
       "4        NaN             NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires requests and pandas\n",
    "\n",
    "# Define search query\n",
    "query = \"climate change\"\n",
    "search_url = f\"https://dataverse.harvard.edu/api/search?q={query}&type=dataset\"\n",
    "\n",
    "# Perform search\n",
    "response = requests.get(search_url)\n",
    "results = response.json()\n",
    "\n",
    "# Convert items to DataFrame\n",
    "items = results['data']['items']\n",
    "df_results = pd.DataFrame(items)\n",
    "\n",
    "# Show key columns\n",
    "#df_results[['name', 'global_id', 'published_at', 'citation']]\n",
    "df_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cf11b-d272-4823-bb22-001368af08b2",
   "metadata": {},
   "source": [
    "##### Search with more than 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ba6ce1-293f-4dd2-90a5-57cf01e33690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>global_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A comprehensive inventory and review of color ...</td>\n",
       "      <td>doi:10.7910/DVN/IFBAZ4</td>\n",
       "      <td>2025-04-22T13:26:32Z</td>\n",
       "      <td>Ripberger, Joseph; Bitterman, Abby; Rosen, Zoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WX18</td>\n",
       "      <td>doi:10.7910/DVN/RHT4ON</td>\n",
       "      <td>2020-02-03T22:11:23Z</td>\n",
       "      <td>Ripberger, Joseph; Silva, Carol; Jenkins-Smith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WX19</td>\n",
       "      <td>doi:10.7910/DVN/MLCJEW</td>\n",
       "      <td>2020-02-03T22:10:59Z</td>\n",
       "      <td>Ripberger, Joseph; Silva, Carol; Jenkins-Smith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WX17</td>\n",
       "      <td>doi:10.7910/DVN/GSTYK4</td>\n",
       "      <td>2020-02-03T22:10:34Z</td>\n",
       "      <td>Ripberger, Joseph; Silva, Carol; Jenkins-Smith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WX20</td>\n",
       "      <td>doi:10.7910/DVN/EWOCUA</td>\n",
       "      <td>2020-09-01T14:03:39Z</td>\n",
       "      <td>Ripberger, Joseph; Krocak, Makenzie; Silva, Ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name               global_id  \\\n",
       "0  A comprehensive inventory and review of color ...  doi:10.7910/DVN/IFBAZ4   \n",
       "1                                               WX18  doi:10.7910/DVN/RHT4ON   \n",
       "2                                               WX19  doi:10.7910/DVN/MLCJEW   \n",
       "3                                               WX17  doi:10.7910/DVN/GSTYK4   \n",
       "4                                               WX20  doi:10.7910/DVN/EWOCUA   \n",
       "\n",
       "           published_at                                           citation  \n",
       "0  2025-04-22T13:26:32Z  Ripberger, Joseph; Bitterman, Abby; Rosen, Zoe...  \n",
       "1  2020-02-03T22:11:23Z  Ripberger, Joseph; Silva, Carol; Jenkins-Smith...  \n",
       "2  2020-02-03T22:10:59Z  Ripberger, Joseph; Silva, Carol; Jenkins-Smith...  \n",
       "3  2020-02-03T22:10:34Z  Ripberger, Joseph; Silva, Carol; Jenkins-Smith...  \n",
       "4  2020-09-01T14:03:39Z  Ripberger, Joseph; Krocak, Makenzie; Silva, Ca...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ripberger\"\n",
    "start = 0\n",
    "per_page = 100  # Max per page is 100\n",
    "all_items = []\n",
    "\n",
    "while True:\n",
    "    search_url = (\n",
    "        f\"https://dataverse.harvard.edu/api/search?\"\n",
    "        f\"q={query}&type=dataset&start={start}&per_page={per_page}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    items = data['data']['items']\n",
    "    all_items.extend(items)\n",
    "    \n",
    "    # Break if fewer than per_page results are returned (i.e., last page)\n",
    "    if len(items) < per_page:\n",
    "        break\n",
    "    start += per_page\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(all_items)\n",
    "df_results[['name', 'global_id', 'published_at', 'citation']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1cf708-914f-48b5-b581-a65ad628c72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many datasets?\n",
    "df_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3ced3-5580-48d3-bfbd-10617e4532db",
   "metadata": {},
   "source": [
    "### Subsetting Your Results\n",
    "\n",
    "Let's say you don't want all of these, but only a subset of related surveys.  For this exercise and the remainder of notebook, we will focus on the yearly waves of the Extreme Weather and Society Survey (WXYY). So let's subset them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0f9d5-63bc-4892-8a4f-88bf2f8550bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset names you want to filter on\n",
    "target_names = [\"WX17\", \"WX18\", \"WX19\", \"WX20\", \"WX21\", \"WX22\", \"WX23\", \"WX24\"]\n",
    "\n",
    "# Subset the dataframe\n",
    "df_subset = df_results[df_results['name'].isin(target_names)]\n",
    "\n",
    "# Display the result\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb964d7",
   "metadata": {},
   "source": [
    "## Step 2: Get Dataset Metadata and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584267c4-9f99-4f97-8881-1c095a6b8f5c",
   "metadata": {},
   "source": [
    "### Getting metadata for a single dataset\n",
    "\n",
    "Let's examine the metadata for one of the datasets in the subset, WX18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row [0] by position\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get dataset metadata\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Display list of files\n",
    "files = metadata_response['data']['latestVersion']['files']\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f684723-3e20-4f78-9766-c90173a6bafc",
   "metadata": {},
   "source": [
    "### Getting metadata for multiple datasets\n",
    "\n",
    "Let's say we want the metadata for all waves.  This cell is set up as a \"loop\", that is, it will loop through all of the files and pick up the metadata information we want.  This time, we will limit the items we want to see (as seen in the \"for f in files\" loop below) and have the results formatted into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db396a02-8713-4d18-ba36-9383808e669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold all file metadata\n",
    "all_files = []\n",
    "\n",
    "# Loop through persistent IDs\n",
    "for pid in df_subset['global_id']:\n",
    "    metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={pid}\"\n",
    "    response = requests.get(metadata_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        metadata = response.json()\n",
    "        files = metadata['data']['latestVersion']['files']\n",
    "        \n",
    "        for f in files:\n",
    "            file_info = {\n",
    "                'dataset_title': metadata['data']['latestVersion']['metadataBlocks']['citation']['fields'][0]['value'],\n",
    "                'file_id': f['dataFile']['id'],\n",
    "                'file_label': f['label'],\n",
    "                'file_size': f['dataFile'].get('filesize', None),\n",
    "                'file_description': f.get('description', ''),\n",
    "                'persistent_id': pid\n",
    "            }\n",
    "            all_files.append(file_info)\n",
    "    \n",
    "    # Be respectful of API limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_files = pd.DataFrame(all_files)\n",
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc278427",
   "metadata": {},
   "source": [
    "## Step 3: Download a File\n",
    "\n",
    "Above, we see that each dataset file (.tab) is accompanied by PDFs of the instrument and a reference report.\n",
    "\n",
    "Let's download the first year of the survey, WX18, which has a file_id of \"3657710\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a111b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File ID for WxEM_Wave1.tab\n",
    "file_id = 3657710\n",
    "\n",
    "# Download directly to memory\n",
    "file_url = f\"https://dataverse.harvard.edu/api/access/datafile/{file_id}?format=original\"\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# Load into pandas directly from memory, assuming comma-delimited content\n",
    "df_18 = pd.read_csv(BytesIO(response.content), sep=',', encoding='ISO-8859-1', engine='python', on_bad_lines='skip')\n",
    "df_18.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d029e2f-c5d5-4a96-b137-8a8d77ba7078",
   "metadata": {},
   "source": [
    "### Examine the dataset\n",
    "\n",
    "Pandas has a number of attributes that can be used to examine characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b165-638f-4f8f-86c7-5baaf39002bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See basic shape of the data (rows, columns)\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af401996-822a-4e92-941f-cd645f729e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all column names\n",
    "df_18.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada7a0f-3378-4f42-a588-383ac6530e3b",
   "metadata": {},
   "source": [
    "#### Variable names\n",
    "\n",
    "Listing the columns shows all the variable names contained in the data.  To integrate with weather data, we are most interested in locating possible ways to join the data.  Typically, geographic variables are a good start.  In this survey, some good possible variables are state, zip, lat/lon.  These are pretty straightforward, but what about \"nws_region\"?  Does it contain WFOs?  Let's examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9502-1d22-4520-a342-1035a7c5b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is nws_region usefl at all?\n",
    "df_18['nws_region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79825058-2652-4f7e-a37e-d2f3e71494a7",
   "metadata": {},
   "source": [
    "##### Unfortunately, \"nws_region\" only has four regions.  Nonetheless, it might be useful at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d675f60",
   "metadata": {},
   "source": [
    "## Step 4: Merge with Weather Data\n",
    "\n",
    "In this example, we will extract warnings/watches/advisories from the Iowa Mesonet for each respondent three days prior to the start of the survey.  I know, that's pretty arbitrary but perhaps we're interested in the impact of the weather on responses to a weather survey.\n",
    "\n",
    "For each respondent, we will use their:\n",
    "\n",
    "1. lat, lon, and begin_date\n",
    "2. Query the IEM for any warnings/watches/advisories issued within 3 days prior to begin_date\n",
    "3. Store or summarize those results (e.g., number of WWAs, types, text, etc.)\n",
    "\n",
    "Please note the API Endpoint (Rest-like) and the structure of the request for those variables:\n",
    "\n",
    "https://mesonet.agron.iastate.edu/vtec/json.php?lon={lon}&lat={lat}&sdate={start}&edate={end}\n",
    "\n",
    "We will use this API call to loop through all 3,000 respondents and gather their information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982baaf3-b0e5-4923-9af9-dc5cdf42695b",
   "metadata": {},
   "source": [
    "### Iowa Mesonet \n",
    "\n",
    "So I'm just going to use lat/lon and the date to directly get watches/warnings/advisories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03230106-daee-42e8-99d1-8e635f846478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure begin_date is in datetime format\n",
    "df_18['begin_date'] = pd.to_datetime(df_18['begin_date'])\n",
    "\n",
    "# New column to store list of WWA names\n",
    "df_18['wwa_names'] = None\n",
    "\n",
    "# Loop through each respondent\n",
    "for idx, row in tqdm(df_18.iterrows(), total=len(df_18)):\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    end_date = row['begin_date']\n",
    "    start_date = end_date - timedelta(days=3)\n",
    "\n",
    "    # Build API URL\n",
    "    url = (\n",
    "        f\"https://mesonet.agron.iastate.edu/json/vtec_events_bypoint.py\"\n",
    "        f\"?lat={lat}&lon={lon}&sdate={start_date.strftime('%Y-%m-%d')}&edate={end_date.strftime('%Y-%m-%d')}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            names = [event['name'] for event in data.get('events', [])]\n",
    "            df_18.at[idx, 'wwa_names'] = names\n",
    "        else:\n",
    "            df_18.at[idx, 'wwa_names'] = []\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for idx={idx}, lat={lat}, lon={lon}: {e}\")\n",
    "        df_18.at[idx, 'wwa_names'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba1ca0-c11e-40cb-8fe0-939eea5e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's examine what we just collected.  We use the attribute \"dropna\" to make sure that there are no rows that empty (i.e., we didn't screw something up).\n",
    "df_18[['lat', 'lon', 'begin_date', 'wwa_names']].dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfd706-64e8-4b5d-8fe8-c1f7c136723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make sure we have the same number of row that we started with\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479299b-22fb-4e6b-9968-58ce1f5ac62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many respondents experienced watches/warnings/advisories?\n",
    "df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb31113-610d-4890-a714-6cd4fee795e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#who received a tornado warning?\n",
    "df_18[df_18['wwa_names'].apply(lambda x: 'Tornado Warning' in x if isinstance(x, list) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fecd87-078a-43f9-88ff-54b947ac89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#who received more than two?\n",
    "df_18[\n",
    "    df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 2)\n",
    "][['lat', 'lon', 'begin_date', 'wwa_names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead4b5-ceb4-4e74-8ab1-bfa05ecd9034",
   "metadata": {},
   "source": [
    "### Quick Visualizations\n",
    "\n",
    "These are quick descriptive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3200f-03e3-4ccd-ba8d-1ee1f1c5e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure lat/lon are float (just in case)\n",
    "df_18['lat'] = pd.to_numeric(df_18['lat'], errors='coerce')\n",
    "df_18['lon'] = pd.to_numeric(df_18['lon'], errors='coerce')\n",
    "\n",
    "# Step 2: Create geometry from lat/lon\n",
    "geometry = [Point(xy) for xy in zip(df_18['lon'], df_18['lat'])]\n",
    "gdf_18 = gpd.GeoDataFrame(df_18, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Step 3: Count WWAs\n",
    "gdf_18['wwa_count'] = gdf_18['wwa_names'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Step 4: Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "gdf_18.plot(column='wwa_count', cmap='OrRd', legend=True, ax=ax, markersize=10)\n",
    "ax.set_title(\"Survey Respondents by Number of WWAs (3 Days Prior)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb67ea-8ce5-4ce5-911f-2fa3f0dee664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and count\n",
    "all_names = df_18['wwa_names'].dropna().explode()\n",
    "top_names = Counter(all_names).most_common(10)\n",
    "\n",
    "# Plot\n",
    "names, counts = zip(*top_names)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(names[::-1], counts[::-1])\n",
    "plt.title(\"Top 10 Most Frequent WWAs\")\n",
    "plt.xlabel(\"Number of Respondents Exposed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b5893-7286-410b-a6c3-594d62c40043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count WWAs per date\n",
    "timeline = (\n",
    "    df_18[['begin_date', 'wwa_names']]\n",
    "    .dropna()\n",
    "    .assign(wwa_count=lambda df: df['wwa_names'].apply(len))\n",
    "    .groupby('begin_date')['wwa_count']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Plot\n",
    "timeline.plot(marker='o', figsize=(10, 4), title='Total WWAs by Survey Date')\n",
    "plt.ylabel('Total Warnings/Watch Events')\n",
    "plt.xlabel('Survey Date')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c6a06-2fd5-48eb-bb08-46818c8aaa24",
   "metadata": {},
   "source": [
    "### Visualizing WWWAs by Survey Reponses\n",
    "\n",
    "For a next step, we might do some quick analyses/visualizations of WWAs by survey response.  Perhaps even make an interactive version with a dropdown of survey responses that a user can play with?\n",
    "\n",
    "Let's say we want to examine how the presence of recent NWS advisories (3 days prior to taking the survey) correlates with survey respondents' perceived risk of that hazard.  How can we do that?\n",
    "\n",
    "Step 1.  Create two groups of respondents, those experienced a WWA prior to the survey and those who did not.\n",
    "Step 2.  Extract risk perception scores for a hazard (For risk_flood: 1-No risk, 2-Low Risk.....5-Extreme risk)\n",
    "Step 3.  Create a comparative visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211e79f-df3e-4b56-9d2d-83c13af46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant flood WWA keywords\n",
    "flood_wwa_keywords = ['Flood Advisory', 'Flood Warning', 'Flash Flood Warning', 'Flash Flood Watch', 'Flood Watch']\n",
    "\n",
    "# Create binary exposure column\n",
    "df_18['flood_wwa_exposure'] = df_18['wwa_names'].apply(\n",
    "    lambda wwas: any(flood_type in wwas for flood_type in flood_wwa_keywords)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084549b3-555b-44c8-86af-c9c840ff4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correct order for risk perception labels\n",
    "ordered_risk_labels = ['No risk', 'Low risk', 'Moderate risk', 'High risk', 'Extreme risk']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(\n",
    "    data=df_18,\n",
    "    x='risk_flood_label',\n",
    "    hue='flood_wwa_exposure',\n",
    "    order=ordered_risk_labels\n",
    ")\n",
    "\n",
    "# Calculate total count per category to get percentages\n",
    "total_counts = df_18.groupby(['risk_flood_label', 'flood_wwa_exposure']).size().reset_index(name='count')\n",
    "category_totals = df_18.groupby('risk_flood_label').size()\n",
    "\n",
    "# Add percentage labels on each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height == 0:\n",
    "        continue  # skip zero-height bars\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    hue_val = p.get_facecolor()  # used just for clarity, not necessary\n",
    "    label = p.get_label()\n",
    "    category = p.get_x() + p.get_width() / 2\n",
    "    risk_label = p.get_x()\n",
    "    \n",
    "    # Get the category label (x-axis tick) associated with this bar\n",
    "    idx = int(p.get_x() + p.get_width() / 2)\n",
    "    category = ordered_risk_labels[int(p.get_x() + p.get_width() / 2)]\n",
    "    \n",
    "    # Get percent value\n",
    "    bar_label = ax.get_xticklabels()[int(x)].get_text()\n",
    "    total = category_totals[bar_label]\n",
    "    percent = 100 * height / total\n",
    "    ax.text(x, height + 1, f'{percent:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Final plot touches\n",
    "plt.xlabel(\"Perceived Flood Risk\")\n",
    "plt.ylabel(\"Number of Respondents\")\n",
    "plt.title(\"Perceived Flood Risk by Exposure to Flood-Related WWAs\")\n",
    "plt.legend(title=\"WWA Exposure\", labels=[\"No\", \"Yes\"])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to PNG.  It will be saved in the directory with this notebook.\n",
    "plt.savefig(\"perceived_flood_risk_by_wwa.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e72242-80ba-4658-9c5c-b89c04c71d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages\n",
    "crosstab = pd.crosstab(df_18['flood_wwa_exposure'], df_18['risk_flood_label'], normalize='index')\n",
    "\n",
    "# Plot\n",
    "crosstab.loc[[True, False]].reindex(columns=ordered_risk_labels).T.plot(\n",
    "    kind='bar', stacked=True, figsize=(10, 6), colormap='viridis'\n",
    ")\n",
    "plt.title('Flood Risk Perception by WWA Exposure (Proportions)')\n",
    "plt.xlabel('Perceived Flood Risk')\n",
    "plt.ylabel('Proportion of Respondents')\n",
    "plt.legend(title='Exposed to Flood WWA', labels=['Yes', 'No'])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to PNG.  It will be saved in the directory with this notebook.\n",
    "plt.savefig(\"perceived_flood_risk_by_wwa.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257784d-91b7-495d-b2a7-87e6b8a29518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't personally like violin plots, but let's take a look\n",
    "sns.violinplot(data=df_18, x='flood_wwa_exposure', y='risk_flood', inner='quartile')\n",
    "plt.xticks([0, 1], ['No WWA Exposure', 'WWA Exposure'])\n",
    "plt.xlabel(\"Exposure to Flood WWA\")\n",
    "plt.ylabel(\"Perceived Flood Risk (1-5)\")\n",
    "plt.title(\"Distribution of Flood Risk Perception by WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b31535-6c89-4c43-935b-f1d5b7934c54",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "We're not limited to visualizations, we could explore regressions, structural equation modeling, etc.  But perhaps it is enough that we've shown how to enrich a dataset that could be saved and used in whatever statistics package they like: R, SAS, SPSS, Stata, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9805ec1-577e-4e29-bf8b-c58571dd9241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60e3b-5a33-4a9f-9d0f-d82366e42625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4dc504-1897-44b6-bf0b-5222f52dae0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
