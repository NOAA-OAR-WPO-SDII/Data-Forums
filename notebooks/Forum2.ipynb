{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff320bf9",
   "metadata": {},
   "source": [
    "# Forum 2 - From Metadata to Insights\n",
    "\n",
    "Author: Jonathon Mote, PhD - Weather Program Office\n",
    "September 2025\n",
    "\n",
    "This tutorial is designed for social scientists who want to explore how their own data might begin to interface with weather and hazard datasets.  The tutorial will provide a quick overview of Jupyter notebooks and tools, some geospatial tools, and API access for environmental and weather data.\n",
    "\n",
    "### Goals:\n",
    "1.  **Work with APIs** to search for, access, and download data programmatically\n",
    "2.  Organize, explore, and download datasets interactively using python libraries like **pandas** and **requests**.\n",
    "3.  Merge survey data with external data from the **Iowa Environmental Mesonet**\n",
    "4.  Apply **geospatial tools** to handle location-based data\n",
    "5.  Create clear, reproducible **visualizations** and **statistical analyses** directly alongside your analysis\n",
    "6.  Document our process in a way that combines code, results, and explanation all in one place  \n",
    "\n",
    "Note: Ensure that the datasets are open access, or you have an API key for restricted access.  For this example, the datasets in this notebook do not require an API key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f3885-3b64-4fd2-a631-338141a481e5",
   "metadata": {},
   "source": [
    "### The Steps We Will Follow\n",
    "\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; gap:14px; margin:14px 0 6px 0;\">\n",
    "\n",
    "  <!-- Step 1 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#d0eff8; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 1: Use an API to explore a repository, view metadata, and \"pull\" social data\n",
    "  </div>\n",
    "  <div style=\"width:2px; height:28px; background:#bfbfbf; border-radius:1px;\"></div>\n",
    "\n",
    "  <!-- Step 2 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#0069af; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 2: Use an API to \"pull\" weather data\n",
    "  </div>\n",
    "  <div style=\"width:2px; height:28px; background:#bfbfbf; border-radius:1px;\"></div>\n",
    "\n",
    "  <!-- Step 3 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#004b98; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 3: Merge weather data and social data\n",
    "  </div>\n",
    "  <div style=\"width:2px; height:28px; background:#bfbfbf; border-radius:1px;\"></div>\n",
    "\n",
    "  <!-- Step 4 -->\n",
    "  <div style=\"width:100%; max-width:900px; height:120px; background:#003087; color:#ffffff; border-radius:12px;\n",
    "              display:flex; align-items:center; justify-content:center; text-align:center;\n",
    "              padding:0 20px; box-sizing:border-box; font-weight:700; line-height:1.3; font-size:20px;\">\n",
    "    Step 4: Visualize and analyze the combined dataset\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88370d06-a6aa-47e8-bb4a-cb4bb3f3b9d9",
   "metadata": {},
   "source": [
    "### Why Do This?\n",
    "\n",
    "In general, the integration of weather and hazard data, and addressing both temporal and spatial resolutions, might allow for a deeper understanding of how people perceive, interpret, and respond to weather and climate risks in relation to the actual meteorological conditions they experience.  Fusing data in this way opens up a range of potential research questions:\n",
    "\n",
    "1. How accurately do people perceive the frequency or severity of extreme weather events in their area?\n",
    "2. Are there socioeconomic or demographic predictors of inaccurate weather risk perception?\n",
    "3. Do people who recall receiving more weather warnings perceive greater weather risk?\n",
    "4. Is there a mismatch between how meteorologists frame risk (as understood by watches and warnings) and how the public interprets it?\n",
    "5. Are individuals who have experienced extreme weather more likely to change their behavior or risk perception?\n",
    "\n",
    "### Think About Your Own Data (Respond in the chat)\n",
    "\n",
    "Thinking about your data, what types of weather-related data might bring additional insights?  What are some questions that you're interested in?  \n",
    "\n",
    "<h2><span style=\"color:red\">Our Research Question Today</span></h2>\n",
    "\n",
    "For the purposes of this tutorial, we will explore the following question: does exposure to watches, warnings, and advisories have an impact on survey responses to weather risk perception.  Specifically, we will focus on flood warnings, watches, and advisories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867feb-2ffc-4d5e-9e14-3d4f277525e9",
   "metadata": {},
   "source": [
    "### Before We Get Started - Workplace Setup (Imports)\n",
    "\n",
    "In Jupyter, there are a large number of python-based \"libraries\" or \"packages\" that help with data loading, transformation, and analysis.  These libraries provide tools that help us do things like make graphs, work with data, or do more complex calculations, like regression.  It's good practice to have all libraries imported at the beginning.  You can always add (even install) libraries as you go along, you just have to rerun the cells (or restart the *kernel* if a new install).  \n",
    "\n",
    "To run a cell, you can go to \"Run\" in the Jupyter menu and select \"Run selected cell\".  However, it is easier to click on the chevron (‚ñ∂Ô∏è) in the editing menu.\n",
    "\n",
    "In this tutorial, the primary libraries we will use are:\n",
    "\n",
    "*Data Handling*\n",
    "\n",
    "- **pandas**: For working with tabular data in DataFrames.  It is commonly imported with an alias (pd), so we don't constantly have to type out pandas.\n",
    "- **requests**: For easily fetching data from web APIs and URLs (using GET, POST, etc).\n",
    "- **timedelta**: Imported from the datetime library, for representing time intervals.\n",
    "- **BytesIO**: Imported from the IO library, for treating in-memory bytes like a file for reading.\n",
    "- **ast**: A python module which can be used for evaluating strings.\n",
    "\n",
    "*Geospatial*\n",
    "\n",
    "- **geopandas**: To work with geospatial data, allowing us to perform spatial operations and handle geometries such as points, polygons, and lines.\n",
    "- **Point**: Imported from Shapely, for creating geometric points for mapping.\n",
    "\n",
    "*Visualization*\n",
    "\n",
    "- **Pyplot**: Imported from MatPlotLib using the alias \"plt\", for making simple customizable  plots and charts.\n",
    "- **Seaborn**: Imported using the alias \"sns\", for making better looking visualizations.\n",
    "  \n",
    "*Utilities*\n",
    "\n",
    "- **time**: Provides functions for working with time, such as measuring durations, pausing executions, and accessing system time.\n",
    "- **tqdm**: Adds progress bars to loops for tracking execution.\n",
    "\n",
    "*Statistical Modeling/Inference*\n",
    "\n",
    "- **scipy.stats.chi2_contingency**: Imported from Scipy, to run a chi-square test of independence to check if two categorical variables are related.\n",
    "- **statsmodels.api as sm**: Imported with the alias \"sm\", it provides tools for statistical models, including logistic regression.\n",
    "- **OrderedModel**: Imported from statsmodels, used for ordinal logistic regression models when outcomes are ordered categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691dcae-b33c-4ab8-917d-da4b7f2de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "# data handling\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#utilities\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#statistical modeling/inference\n",
    "from scipy.stats import chi2_contingency        \n",
    "import statsmodels.api as sm                    \n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdde11",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0085ca66; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 1\n",
    "</div>\n",
    "\n",
    "## Step 1: Use an API to explore a repository, view metadata, and download data\n",
    "\n",
    "In this step, we will explore API access to a data repository, the Harvard Dataverse.  An API (Application Programming Interface) is just a set of rules and tools that allows different software to communicate and interact with each other.  In this case, we want our Jupyter notebook to interact with the server for information on datasets.  We use the python library \"Requests\" to simplify and automate our requests, and Dataverse returns what we requested (hopefully), typically in a format called JSON.  We then use Pandas to transform the JSON in a dataframe, making the results easier to read and manipulate.  \n",
    "\n",
    "- **Note**: Not all APIs are created equally and there might be differences across repositories.  Check each API's documentation for how to get started, authentication, search and data access, and more.  For Harvard's Dataverse, the [Dataverse API Guide](https://guides.dataverse.org/en/latest/api/index.html) is a comprehensive, up-to-date documentation for all operations in Harvard‚Äôs Dataverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b78722-de05-4332-bd49-265d741a5cc0",
   "metadata": {},
   "source": [
    "##### Simple search for 10 results\n",
    "\n",
    "By default, the Harvard Dataverse only returns 10 results per search request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037550f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires requests and pandas\n",
    "\n",
    "# Define search query\n",
    "query = \"ripberger\"\n",
    "search_url = f\"https://dataverse.harvard.edu/api/search?q={query}&type=dataset\"\n",
    "\n",
    "# Perform search\n",
    "response = requests.get(search_url)\n",
    "results = response.json()\n",
    "\n",
    "# Convert items to DataFrame\n",
    "items = results['data']['items']\n",
    "df_results = pd.DataFrame(items)\n",
    "\n",
    "# Show key columns\n",
    "#df_results[['name', 'global_id', 'published_at', 'citation']]\n",
    "df_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cf11b-d272-4823-bb22-001368af08b2",
   "metadata": {},
   "source": [
    "##### Search with more than 10 results\n",
    "\n",
    "We can make an API call that goes beyond the 10 result limit by creating a loop.  The \"while True\" statement will continue running (10 results at a time) until there are no results remaining.  We collect all of the results in one list using the \"extend\" command.  Finally, we can limit the dataframe to only view a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba6ce1-293f-4dd2-90a5-57cf01e33690",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ripberger\"\n",
    "start = 0\n",
    "per_page = 20  # Max per page is 100\n",
    "all_items = []\n",
    "\n",
    "while True:\n",
    "    search_url = (\n",
    "        f\"https://dataverse.harvard.edu/api/search?\"\n",
    "        f\"q={query}&type=dataset&start={start}&per_page={per_page}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    items = data['data']['items']\n",
    "    all_items.extend(items)\n",
    "    \n",
    "    # Break if fewer than per_page results are returned (i.e., last page)\n",
    "    if len(items) < per_page:\n",
    "        break\n",
    "    start += per_page\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(all_items)\n",
    "df_results[['name', 'global_id', 'published_at', 'citation']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cf708-914f-48b5-b581-a65ad628c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many datasets?  Each row (first number) represents a dataset.\n",
    "df_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3ced3-5580-48d3-bfbd-10617e4532db",
   "metadata": {},
   "source": [
    "### Subsetting Our Results\n",
    "\n",
    "Let's say we don't want all of these, but only a subset of related surveys.  For this step, and the remainder of notebook, we will focus on the yearly waves of the Extreme Weather and Society Survey (WXYY). So let's subset them the dataframe from the earlier API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0f9d5-63bc-4892-8a4f-88bf2f8550bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset names you want to filter on\n",
    "target_names = [\"WX17\", \"WX18\", \"WX19\", \"WX20\", \"WX21\", \"WX22\", \"WX23\", \"WX24\"]\n",
    "\n",
    "# Subset the dataframe\n",
    "df_subset = df_results[df_results['name'].isin(target_names)]\n",
    "\n",
    "# Display the result\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb964d7",
   "metadata": {},
   "source": [
    "## Step 1a: Get Dataset Metadata and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584267c4-9f99-4f97-8881-1c095a6b8f5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Getting metadata for a single dataset\n",
    "\n",
    "Let's examine the metadata for one of the datasets in the subset, WX18.  There will are often two sets of metadata: file-level (specific to the repository) and data-level (that describes the data---the topic of yesterday's forum).  In this example, we will first look at the file-level metadaa that the Dataverse uses.  Next, we will pull the full dataset metadata.  The results will be in JSON (which was discussed yesterday), we will convert that to a flat file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85380a77-04f2-469a-b550-e34c16b25259",
   "metadata": {},
   "source": [
    "### File level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row [0] by position\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get dataset metadata\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Display list of files\n",
    "files = metadata_response['data']['latestVersion']['files']\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d8ce9-6a2d-4cc8-a053-5e10f8a04a0e",
   "metadata": {},
   "source": [
    "### Full dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f4f8b-9f6a-4485-955c-f81dde9bc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract persistent ID (DOI) from the first row\n",
    "persistent_id = df_subset.iloc[0]['global_id']\n",
    "\n",
    "# Get full dataset metadata (latest version)\n",
    "metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/versions/:latest?persistentId={persistent_id}\"\n",
    "metadata_response = requests.get(metadata_url).json()\n",
    "\n",
    "# Display the JSON\n",
    "metadata_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7019893-9ffe-47f5-97e9-09bca1ceb354",
   "metadata": {},
   "source": [
    "### Transform the results from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dd163-71e0-4c87-a24b-5bf42d3609d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten JSON into a single row using json_normalize\n",
    "df_meta = pd.json_normalize(metadata_response)\n",
    "\n",
    "# Transpose so keys become a column and values another\n",
    "df_meta_t = df_meta.T.reset_index()\n",
    "df_meta_t.columns = [\"field\", \"value\"]\n",
    "\n",
    "df_meta_t.head(30)  # show first 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f684723-3e20-4f78-9766-c90173a6bafc",
   "metadata": {},
   "source": [
    "### Getting metadata for multiple datasets\n",
    "\n",
    "Let's say we want the file-level metadata for all waves.  Against, we set up a loop call, to loop through all of the files using the persistent ID (DOI) and pick up the metadata information we want.  This time, we will limit the items we want to see (as seen in the \"for f in files\" loop below) and have the results formatted into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db396a02-8713-4d18-ba36-9383808e669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold all file metadata\n",
    "all_files = []\n",
    "\n",
    "# Loop through persistent IDs\n",
    "for pid in df_subset['global_id']:\n",
    "    metadata_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/?persistentId={pid}\"\n",
    "    response = requests.get(metadata_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        metadata = response.json()\n",
    "        files = metadata['data']['latestVersion']['files']\n",
    "        \n",
    "        for f in files:\n",
    "            file_info = {\n",
    "                'dataset_title': metadata['data']['latestVersion']['metadataBlocks']['citation']['fields'][0]['value'],\n",
    "                'file_id': f['dataFile']['id'],\n",
    "                'file_label': f['label'],\n",
    "                'file_size': f['dataFile'].get('filesize', None),\n",
    "                'file_description': f.get('description', ''),\n",
    "                'persistent_id': pid\n",
    "            }\n",
    "            all_files.append(file_info)\n",
    "    \n",
    "    # Be respectful of API limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_files = pd.DataFrame(all_files)\n",
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc278427",
   "metadata": {},
   "source": [
    "## Step 1b: Download a File\n",
    "\n",
    "Above, we see that each dataset file (.tab) is accompanied by PDFs of the instrument and a reference report.\n",
    "\n",
    "Let's download the dataset file (.tab) for first year of the survey, WX18, which has a file_id of \"3657710\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a111b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File ID for WxEM_Wave1.tab\n",
    "file_id = 3657710\n",
    "\n",
    "# Download directly to memory\n",
    "file_url = f\"https://dataverse.harvard.edu/api/access/datafile/{file_id}?format=original\"\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# Load into pandas directly from memory, assuming comma-delimited content\n",
    "df_18 = pd.read_csv(BytesIO(response.content), sep=',', encoding='ISO-8859-1', engine='python', on_bad_lines='skip')\n",
    "df_18.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d029e2f-c5d5-4a96-b137-8a8d77ba7078",
   "metadata": {},
   "source": [
    "### Examine the dataset\n",
    "\n",
    "Pandas has a number of attributes that can be used to examine characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b165-638f-4f8f-86c7-5baaf39002bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See basic shape of the data (rows, columns).  Each row represents a respondent.\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada7a0f-3378-4f42-a588-383ac6530e3b",
   "metadata": {},
   "source": [
    "#### Variable names\n",
    "\n",
    "Listing the columns shows all the variable names contained in the data.  To integrate with weather data, we are most interested in locating possible ways to join the data.  Typically, geographic variables are a good start. \n",
    "\n",
    "<h3><span style=\"color:red\">After we run the next cell, do you see any geographic variables that might be useful?</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0012e2-7334-492d-8dd6-9026a265fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all column names\n",
    "df_18.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5994548-d663-41a2-bd1e-a3922742595a",
   "metadata": {},
   "source": [
    "In this survey, some good possible variables are state, zip, and lat/lon. These are pretty straightforward, but what about \"nws_region\"? Does it contain WFOs? Let's examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9502-1d22-4520-a342-1035a7c5b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is nws_region usefl at all?\n",
    "df_18['nws_region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79825058-2652-4f7e-a37e-d2f3e71494a7",
   "metadata": {},
   "source": [
    "##### Unfortunately, \"nws_region\" only has four regions.  Nonetheless, it might be useful for a different research question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394a574-7183-41b4-bb62-f29394ca654b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fb1e1eff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Stop!  Take a break! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d675f60",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0069afff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 2\n",
    "</div>\n",
    "\n",
    "## Step 2: Use an API to download weather data\n",
    "\n",
    "First, we demonstrate how the Iowa Mesonet API can be used to collect weather alerts for each survey respondent.  Since calling an API can be slow and depends on internet access, we have already pre-downloaded the weather data needed for this analysis.  \n",
    "\n",
    "Next, we will take this pre-loaded weather data and merge it with the survey responses so that each person‚Äôs record includes both their answers and the relevant weather alerts.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982baaf3-b0e5-4923-9af9-dc5cdf42695b",
   "metadata": {},
   "source": [
    "### Iowa Mesonet API - DO NOT RUN AT THIS TIME\n",
    "\n",
    "The code below is doing alot of work, going through the survey data **one person at a time** and asking the Iowa Mesonet API:  \n",
    "\n",
    "‚ÄúGiven this person‚Äôs location and survey date, what watches, warnings, or advisories (WWAs) were active in the few days leading up to that date?‚Äù  \n",
    "\n",
    "Here‚Äôs what happens step by step:  \n",
    "\n",
    "1. **Make sure the date is in the right format.**  \n",
    "   The `begin_date` column is converted into a standard date format so the computer can work with it.  \n",
    "\n",
    "2. **Prepare a place to store the results.**  \n",
    "   A new column called `wwa_names` is added to the survey data. This will eventually hold a *list* of weather alerts for each person.  \n",
    "\n",
    "3. **Go through each respondent one by one.**  \n",
    "   For each person, we:  \n",
    "   - Look up their latitude, longitude, and survey date.  \n",
    "   - Define a **3-day window** before their survey date (so we catch recent alerts).  \n",
    "   - Build a request to the Iowa Mesonet API using their location and dates.  \n",
    "\n",
    "\n",
    "4. **Ask the API for weather alerts.**  \n",
    "   - If the API responds successfully, we pull out the names of any alerts and save them in that person‚Äôs row.  \n",
    "   - If something goes wrong (bad connection, no data, etc.), we save an *empty list* for that person.  \n",
    "\n",
    "By the end, the survey dataset (`df_18`) has a new column called `wwa_names` that tells us which WWAs (if any) each respondent experienced around the time of their survey.  \n",
    "\n",
    "**Analogy:** This is like calling a weather hotline for each person‚Äôs hometown and writing down any recent alerts next to their name in the survey spreadsheet.  \n",
    "\n",
    "Please note the API Endpoint (Rest-like) and the structure of the request for those variables:\n",
    "\n",
    "https://mesonet.agron.iastate.edu/vtec/json.php?lon={lon}&lat={lat}&sdate={start}&edate={end}\n",
    "\n",
    "Remember, you can always check the API documentation for guidance: the [Iowa Mesonet API Guide](https://mesonet.agron.iastate.edu/api/).\n",
    "\n",
    "**Note:  We will not run this API call during the webinar because it takes about 20-30 minutes to download the data. If you want to try this after the webinar, just remove the first line ('''python) and run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03230106-daee-42e8-99d1-8e635f846478",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''python\n",
    "\n",
    "# Make sure begin_date is in datetime format\n",
    "df_18['begin_date'] = pd.to_datetime(df_18['begin_date'])\n",
    "\n",
    "# New column to store list of WWA names\n",
    "df_18['wwa_names'] = None\n",
    "\n",
    "# Loop through each respondent\n",
    "for idx, row in tqdm(df_18.iterrows(), total=len(df_18)):\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    end_date = row['begin_date']\n",
    "    start_date = end_date - timedelta(days=3)\n",
    "\n",
    "    # Build API URL with small buffer\n",
    "    url = (\n",
    "        f\"https://mesonet.agron.iastate.edu/json/vtec_events_bypoint.py\"\n",
    "        f\"?lat={lat}&lon={lon}\"\n",
    "        f\"&sdate={start_date.strftime('%Y-%m-%d')}&edate={end_date.strftime('%Y-%m-%d')}\"\n",
    "        f\"&buffer=0.1\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            names = [event['name'] for event in data.get('events', [])]\n",
    "            df_18.at[idx, 'wwa_names'] = names\n",
    "        else:\n",
    "            df_18.at[idx, 'wwa_names'] = []\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for idx={idx}, lat={lat}, lon={lon}: {e}\")\n",
    "        df_18.at[idx, 'wwa_names'] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776736c5-9b2e-42e3-9f4d-91fea28a6d37",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#004b98ff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 3\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Step 3 - Joining the Survey Data with Weather Alerts\n",
    "\n",
    "Prior to the webinar, the data we needed was downloaded.  This dataset ('wwa_by_pid') includes the respondent identifier ('p_id') and a column with watches, warnings, and advisories ('wwa_names').  At this point, we have two different tables of data that we need to merge:\n",
    "\n",
    "- **Survey data** (df_18): this has all the responses from people, including their p_id (a unique identifier for each person).\n",
    "\n",
    "- **Weather alerts data** (lk): this has just two columns ‚Äî the same p_id, and the list of watches, warnings, or advisories (wwa_names) that each person experienced.\n",
    "\n",
    "We're going to merge both tables on p_id, which is basically doing the following: \n",
    "\n",
    "**‚ÄúFor each person (p_id) in the survey data, look up their matching p_id in the weather file and bring in the weather alerts column (wwa_names).‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6aaa1f-d227-45e6-afc8-bf0cd0c798e6",
   "metadata": {},
   "source": [
    "##### If you ran the API call in Step 2, you should skip the next two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0489d-353f-497c-b1cc-2292f5b61e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have run the API, you should skip this cell\n",
    "\n",
    "# Read in the saved lookup file\n",
    "lk = pd.read_csv(\"../data/wwa_by_pid.csv\")\n",
    "\n",
    "# Merge with the original df_18 on 'p_id'\n",
    "df_18 = df_18.merge(lk, on=\"p_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5b7b4-93ba-4c44-ad61-73f68d0d2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use `ast` to convert the `wwa_names` column from text back into real Python lists so we can work with them.  \n",
    "df_18['wwa_names'] = df_18['wwa_names'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b52e5a-dc51-4fda-9531-51307bd8c0e8",
   "metadata": {},
   "source": [
    "##### If you ran the API call in Step 2, you can resume here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69d50-18ea-496e-9ffa-3e982f319211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the new, combined dataset\n",
    "df_18.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba1ca0-c11e-40cb-8fe0-939eea5e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's examine what we just collected.  We use the attribute \"dropna\" to make sure that there are no rows that are empty (i.e., we didn't screw something up).\n",
    "df_18[['lat', 'lon', 'begin_date', 'wwa_names']].dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfd706-64e8-4b5d-8fe8-c1f7c136723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make sure we have the same number of row that we started with.  It should be 3,000\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479299b-22fb-4e6b-9968-58ce1f5ac62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many respondents experienced watches/warnings/advisories?\n",
    "df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fecd87-078a-43f9-88ff-54b947ac89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#who received more than two?\n",
    "df_18[\n",
    "    df_18['wwa_names'].apply(lambda x: isinstance(x, list) and len(x) > 2)\n",
    "][['lat', 'lon', 'begin_date', 'wwa_names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1bd8-26cb-446d-a4e7-900ee14f6e40",
   "metadata": {},
   "source": [
    "### Step 3a: Exporting the Data for Use Outside Jupyter\n",
    "\n",
    "Now that we have a merged dataframe (`df_18`) with both survey responses and weather alerts, it is possible to save and download in different formats for use in other tools and workflows.  We won't run them here, but I'll provide the code snippets to view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8f528-2648-4ce9-b358-46ee2b7ebe43",
   "metadata": {},
   "source": [
    "- **CSV (general use, Excel, R, SAS, SPSS, STATA.)**\n",
    "\n",
    "df_18.to_csv(\"survey_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00482200-ba23-48de-aba7-00f5eabfbe04",
   "metadata": {},
   "source": [
    "- **R (as .RDS file for use with readRDS() in R)**\n",
    "\n",
    "df_18.to_pickle(\"survey_weather.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead4b5-ceb4-4e74-8ab1-bfa05ecd9034",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#003087ff; color:white; padding:20px; border-radius:10px; text-align:center; font-size:28px; font-weight:bold;\">\n",
    "  Step 4\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Exploring the Combined Dataset\n",
    "\n",
    "After you download the new dataset, you could easily just jump back into your regular workflow.  But let's say we want to continue in Jupyter to quickly visualize and analyze the data to examine the data for insights.  Below we will look at the following:\n",
    "\n",
    "- ***Quick Visualizations***\n",
    "- ***Visualizing WWAs by Survey Responses***\n",
    "- ***Quick Statistical Analyses***\n",
    "- ***A Bit More Involved Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d2172-d491-44ee-8e14-53844f93ce9f",
   "metadata": {},
   "source": [
    "#### Step 4a: Quick visualizations\n",
    "\n",
    "In this step, we will be developing the following visualizations.\n",
    "\n",
    "1. A geomap of survey respondents by number of watches and warnings 3 days prior to the survey.\n",
    "2. A bar chart showing frequency of types of watches and warnings.\n",
    "3. A line graph showing frequency of types of watches and warning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3200f-03e3-4ccd-ba8d-1ee1f1c5e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure lat/lon are float (just in case)\n",
    "df_18['lat'] = pd.to_numeric(df_18['lat'], errors='coerce')\n",
    "df_18['lon'] = pd.to_numeric(df_18['lon'], errors='coerce')\n",
    "\n",
    "# Step 2: Create geometry from lat/lon\n",
    "geometry = [Point(xy) for xy in zip(df_18['lon'], df_18['lat'])]\n",
    "gdf_18 = gpd.GeoDataFrame(df_18, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Step 3: Count WWAs\n",
    "gdf_18['wwa_count'] = gdf_18['wwa_names'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Step 4: Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "gdf_18.plot(column='wwa_count', cmap='OrRd', legend=True, ax=ax, markersize=10)\n",
    "ax.set_title(\"Survey Respondents by Number of WWAs (3 Days Prior)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb67ea-8ce5-4ce5-911f-2fa3f0dee664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and count\n",
    "all_names = df_18['wwa_names'].dropna().explode()\n",
    "top_names = Counter(all_names).most_common(10)\n",
    "\n",
    "# Plot\n",
    "names, counts = zip(*top_names)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(names[::-1], counts[::-1])\n",
    "plt.title(\"Top 10 Most Frequent WWAs\")\n",
    "plt.xlabel(\"Number of Respondents Exposed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b5893-7286-410b-a6c3-594d62c40043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count WWAs per date\n",
    "timeline = (\n",
    "    df_18[['begin_date', 'wwa_names']]\n",
    "    .dropna()\n",
    "    .assign(wwa_count=lambda df: df['wwa_names'].apply(len))\n",
    "    .groupby('begin_date')['wwa_count']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Plot\n",
    "timeline.plot(marker='o', figsize=(10, 4), title='Total WWAs by Survey Date')\n",
    "plt.ylabel('Total Warnings/Watch Events')\n",
    "plt.xlabel('Survey Date')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c6a06-2fd5-48eb-bb08-46818c8aaa24",
   "metadata": {},
   "source": [
    "### Step 4b: Visualizing WWWAs by Survey Reponses\n",
    "\n",
    "For a next step, we might do some quick analyses/visualizations of WWAs by survey response.  Perhaps even make an interactive version with a dropdown of survey responses that a user can play with?\n",
    "\n",
    "Let's say we want to examine how the presence of recent NWS advisories (3 days prior to taking the survey) correlates with survey respondents' perceived risk of that hazard.  How can we do that?\n",
    "\n",
    "Step 1.  Create two groups of respondents, those experienced a WWA prior to the survey and those who did not.\n",
    "Step 2.  Extract risk perception scores for a hazard (For risk_flood: 1-No risk, 2-Low Risk.....5-Extreme risk)\n",
    "Step 3.  Create comparative visualizations\n",
    "\n",
    "For these visualizations, we will focus on exposure to flood watches and warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211e79f-df3e-4b56-9d2d-83c13af46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prep: robust WWA exposure + ordered risk labels ---\n",
    "\n",
    "# 1) robust flood_wwa_exposure (handles NaN/non-lists)\n",
    "flood_wwa_keywords = ['Flood Advisory', 'Flood Warning', 'Flash Flood Warning', 'Flash Flood Watch', 'Flood Watch']\n",
    "\n",
    "def has_flood_wwa(wwas):\n",
    "    if isinstance(wwas, (list, tuple, set)):\n",
    "        return any(k in wwas for k in flood_wwa_keywords)\n",
    "    return False\n",
    "\n",
    "df_18['flood_wwa_exposure'] = df_18['wwa_names'].apply(has_flood_wwa)\n",
    "\n",
    "# 2) numeric -> labeled flood risk (ordered categorical)\n",
    "label_map = {1: 'No risk', 2: 'Low risk', 3: 'Moderate risk', 4: 'High risk', 5: 'Extreme risk'}\n",
    "ordered_risk_labels = list(label_map.values())\n",
    "\n",
    "# Coerce to numeric, map to labels, and set categorical order\n",
    "df_18['risk_flood_num'] = pd.to_numeric(df_18.get('risk_flood'), errors='coerce')\n",
    "df_18['risk_flood_label'] = pd.Categorical(\n",
    "    df_18['risk_flood_num'].map(label_map),\n",
    "    categories=ordered_risk_labels,\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084549b3-555b-44c8-86af-c9c840ff4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(\n",
    "    data=df_18,\n",
    "    x='risk_flood_label',\n",
    "    hue='flood_wwa_exposure',\n",
    "    order=ordered_risk_labels,\n",
    "    hue_order=[False, True]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Perceived Flood Risk\")\n",
    "plt.ylabel(\"Number of Respondents\")\n",
    "plt.title(\"Perceived Flood Risk by Exposure to Flood-Related WWAs\")\n",
    "plt.legend(title=\"WWA Exposure\", labels=[\"No\", \"Yes\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e72242-80ba-4658-9c5c-b89c04c71d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab -> proportions by exposure\n",
    "crosstab = pd.crosstab(\n",
    "    df_18['flood_wwa_exposure'],\n",
    "    df_18['risk_flood_label'],\n",
    "    normalize='index'\n",
    ")\n",
    "\n",
    "# ensure consistent order of rows/columns even if some levels are missing\n",
    "crosstab = crosstab.reindex(index=[False, True], columns=ordered_risk_labels)\n",
    "\n",
    "ax = crosstab.T.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "\n",
    "plt.title('Flood Risk Perception by WWA Exposure (Proportions)')\n",
    "plt.xlabel('Perceived Flood Risk')\n",
    "plt.ylabel('Proportion of Respondents')\n",
    "plt.legend(title='Exposed to Flood WWA', labels=['No', 'Yes'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257784d-91b7-495d-b2a7-87e6b8a29518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't personally like violin plots, but let's take a look\n",
    "sns.violinplot(data=df_18, x='flood_wwa_exposure', y='risk_flood', inner='quartile')\n",
    "plt.xticks([0, 1], ['No WWA Exposure', 'WWA Exposure'])\n",
    "plt.xlabel(\"Exposure to Flood WWA\")\n",
    "plt.ylabel(\"Perceived Flood Risk (1-5)\")\n",
    "plt.title(\"Distribution of Flood Risk Perception by WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b31535-6c89-4c43-935b-f1d5b7934c54",
   "metadata": {},
   "source": [
    "## Step 4C: Quick Statistical Analyses\n",
    "\n",
    "We're not limited to visualizations, we can explore a number of statistical procedures to really test our question of whether exposure to warnings and watches has an impact on survey responses.  For this tutorial, we will quickly conduct the following:\n",
    "\n",
    "1. Chi-Square: To evaluate whether exposure to flood-related warnings and watches and perceived flood risk categories are independent, with the p-value indicating if the observed relationship is statistically significant.\n",
    "2. Logistic Regression: To evaluate whether exposure to flood-related warnings and watches increases the odds of respondents reporting high or extreme flood risk compared to lower risk levels.\n",
    "3. Ordinal Logistic Regression: To evaluate whether exposure to flood-related warnings and watches shifts respondents toward higher categories of perceived flood risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48102616-f3ad-47b9-a863-48aff2a5efd3",
   "metadata": {},
   "source": [
    "### 1. Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9805ec1-577e-4e29-bf8b-c58571dd9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table: WWA exposure vs. risk perception\n",
    "contingency = pd.crosstab(df_18['flood_wwa_exposure'], df_18['risk_flood_label'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\"Chi-square test\")\n",
    "print(f\"Chi2 = {chi2:.2f}, df = {dof}, p = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d836569-37c4-497e-9939-431d6ac8fdc0",
   "metadata": {},
   "source": [
    "This suggests that the distribution of flood risk perceptions is not independent of WWA exposure ‚Äî in other words, people who were exposed to flood-related WWAs responded differently (in terms of risk levels) than those who were not exposed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3943fc-95ab-4b32-ace2-8a0f376bf859",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60e3b-5a33-4a9f-9d0f-d82366e42625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary outcome: high risk (‚â•4) vs. lower\n",
    "df_18['high_risk'] = df_18['risk_flood_num'] >= 4\n",
    "\n",
    "# Predictor: WWA exposure (cast to int)\n",
    "X = sm.add_constant(df_18['flood_wwa_exposure'].astype(int))\n",
    "y = df_18['high_risk'].astype(int)\n",
    "\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2c04a-7e01-4624-aea1-e55ed519613c",
   "metadata": {},
   "source": [
    "This suggests that respondents exposed to a flood WWA had significantly higher odds (about 39% greater, exp(0.3326) ‚âà 1.39) of reporting high or extreme flood risk compared to those not exposed, though the overall model explains only a small share of variation in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c6f48-4a63-4943-8ee3-71812abf60ba",
   "metadata": {},
   "source": [
    "### 3. Ordinal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4dc504-1897-44b6-bf0b-5222f52dae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs to avoid issues\n",
    "df_ord = df_18.dropna(subset=['risk_flood_num', 'flood_wwa_exposure'])\n",
    "\n",
    "# Predictor must be numeric (int)\n",
    "X = df_ord[['flood_wwa_exposure']].astype(int)\n",
    "\n",
    "# Outcome is ordered categories (numeric risk levels already ordered 1‚Äì5)\n",
    "y = df_ord['risk_flood_num']\n",
    "\n",
    "# Fit ordinal logistic regression\n",
    "mod = OrderedModel(y, X, distr='logit')\n",
    "res = mod.fit(method='bfgs')\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262f9dd-e592-410f-9d1b-9b053e8c4004",
   "metadata": {},
   "source": [
    "This suggests that respondents exposed to flood-related warnings and watches had a significantly higher likelihood of placing themselves in higher flood risk perception categories (coef = 0.296, p < 0.001), indicating a consistent upward shift in perceived risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6a768-069c-4eda-8c8f-0bcdee2245de",
   "metadata": {},
   "source": [
    "## A Bit More Involved - Did WWAs *Really* Affect Risk Perception?\n",
    "\n",
    "So it appears that exposure to watches and warnings has a statistically significant impact on survey responses.  But by how much? To answer this, we can look at the **predicted probabilities** of survey responses.  This shows us how much exposure to a WWA changes the likelihood of a respondent reporting ‚ÄúHigh‚Äù or ‚ÄúExtreme‚Äù flood risk.  \n",
    "\n",
    "To do this, the code below creates two simple scenarios: one where a person had no flood alert (0) and one where they did (1).  It then uses our statistical model to predict the probability of each risk category under those two scenarios.  The results are labeled clearly as ‚ÄúNo Exposure‚Äù and ‚ÄúExposure,‚Äù giving us a side-by-side view.  \n",
    "\n",
    "**Analogy:** It‚Äôs like asking, ‚ÄúWhat would the risk look like if nobody had a flood alert?‚Äù and then,  ‚ÄúWhat would the risk look like if everyone had a flood alert?‚Äù ‚Äî and comparing the two answers side by side.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3bdae-4502-4d07-8e19-5389cbb76050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two scenarios: no exposure (0) and exposure (1)\n",
    "scenarios = pd.DataFrame({\n",
    "    'flood_wwa_exposure': [0, 1]\n",
    "})\n",
    "\n",
    "# Predict probabilities for each risk category\n",
    "pred_probs = res.predict(scenarios)\n",
    "\n",
    "# Attach labels\n",
    "pred_probs.index = ['No Exposure', 'Exposure']\n",
    "pred_probs.columns = [f\"Risk {c}\" for c in pred_probs.columns]\n",
    "\n",
    "pred_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a547651-6f6e-42a7-9d62-5716cc99e669",
   "metadata": {},
   "source": [
    "## A possible interpretation\n",
    "\n",
    "Respondents who were exposed to a flood WWA were less likely to report no risk (10% ‚Üí 8%) or low risk (32% ‚Üí 28%), and more likely to report higher levels of risk perception, particularly at the ‚ÄúHigh‚Äù (15% ‚Üí 18%) and ‚ÄúExtreme‚Äù (9% ‚Üí 11%) categories. While the percentage point changes may look modest, they indicate a clear upward shift in perceived flood risk among those who received WWAs.  In other words, I think it is possible to say that exposure to a flood warning nudged people away from saying ‚Äòno risk‚Äô and toward saying ‚Äòhigh or extreme risk.  However, with only 3,000 responses, I probably wouldn't.  But, as the academics say, this requires further study. üòâ\n",
    "\n",
    "Now let's take a look at a visualization of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d88ad-5778-4c2a-8c1f-88a3a3da1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see HOW the probability distribution shift\n",
    "ax = pred_probs.T.plot(kind='bar', figsize=(10,6))\n",
    "plt.title(\"Predicted Flood Risk Perception by WWA Exposure\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Perceived Flood Risk Level\")\n",
    "plt.legend(title=\"WWA Exposure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e86aa-c61b-4c40-ad08-13ec49961f36",
   "metadata": {},
   "source": [
    "## Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5570c5-de5c-4d01-8741-1fb8b8f7e3ba",
   "metadata": {},
   "source": [
    "In this tutorial, we explored how to bring together **social survey data** and **weather warning data** to better understand how hazard information influences perceptions of risk. Using the Jupyter Notebook environment, you learned how to:  \n",
    "\n",
    "- **Work with APIs** to search for, access, and download data programmatically  \n",
    "- Organize and explore datasets interactively using python libraries like **pandas** and **requests**.  \n",
    "- Merge survey data with external data from the **Iowa Environmental Mesonet**  \n",
    "- Apply **geospatial tools** to handle location-based data  \n",
    "- Create clear, reproducible **visualizations** and **statistical analyses** directly alongside your analysis  \n",
    "- Document your process in a way that combines code, results, and explanation all in one place  \n",
    "\n",
    "With this walkthrough, you‚Äôve seen how Jupyter can serve as both a **research lab and a communication tool**‚Äîa space where you can work with data, analyze the data, visualize the data and results, and explain what you found.  \n",
    "\n",
    "### Moving Forward  \n",
    "- Try adapting this workflow to other survey topics (e.g., heat, drought, tornado risk)  \n",
    "- Explore additional Mesonet or NOAA APIs to enrich your analysis with different kinds of data  \n",
    "- Use Jupyter notebooks to build **reproducible reports**, where readers can see not just your conclusions but also the steps you took to get there  \n",
    "- Share your notebooks with collaborators as a way to make your analysis **transparent and interactive**  \n",
    "\n",
    "Ultimately, the key takeaway is that with just a few tools‚Äî**APIs, pandas, geospatial libraries, and Jupyter notebooks**‚Äîyou can connect diverse datasets, analyze them in context, and tell meaningful data stories about risk and society.  \n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for following along!**  \n",
    "We encourage you to take this workflow and apply it to your own research questions about weather, risk, and society‚Äîthe more you explore, the more insights you‚Äôll uncover.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
